{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global temperature in 2024 with regard to climate change and internal variability\n",
    "Christophe Cassou & Aurélien Liné\n",
    "\n",
    "In order to plot years 2022, 2023, and 2024, this Notebook needs to be excecuted 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "from importlib import reload\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import patches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib import colors as mclrs\n",
    "from matplotlib import ticker as mtick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'tas'\n",
    "table = 'Amon'\n",
    "model = None\n",
    "experiment = ['hist-ssp126', 'hist-ssp245', 'hist-ssp370', 'hist-ssp585']\n",
    "# Restrictions on SMILES\n",
    "only_one_member = False ### If True, keeps only the r1 member for each couple of model-experiment\n",
    "ensemble_size_min = 1 # 1 5\n",
    "# Targeted values and years\n",
    "targets = {'yr':  {'year': '2024', 'GWL': 1.36}}\n",
    "observations = {'WMO': {'yr': 1.52}}\n",
    "year_min = 2024\n",
    "# Pre-industrial period used to compute GWL from\n",
    "piStr = '1850'; piEnd = '1900'\n",
    "# Reference period to use that can differ from the PI (for example 1991-2020)\n",
    "refStr = '1850'; refEnd = '1900'\n",
    "gwl_method = 'cross' # cross state\n",
    "gwl_interval = 0.01 ### Relevant if gwl_method == 'state'\n",
    "window_size = 11 \n",
    "# Constraint on interanual variability\n",
    "constraint = 'obs_rea' # obs rea obs_rea\n",
    "stats_str = '1950'\n",
    "stats_end = '2014'\n",
    "# Drivers of internal variability\n",
    "driver = ['amv', 'nino34']\n",
    "# Bootstrap\n",
    "bootstrap = True\n",
    "n_boot = 1000\n",
    "confidence_range = .9\n",
    "# Plotting parameters\n",
    "show_gauss = True\n",
    "extension = '.pdf'\n",
    "\n",
    "dataDir = 'Data/'\n",
    "outDir = 'Outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute year 2022\n",
    "\"\"\"\n",
    "targets = {'yr':  {'year': '2022', 'GWL': 1.26}}\n",
    "observations = {'WMO': {'yr': 1.15}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute year 2023\n",
    "\"\"\"\n",
    "targets = {'yr':  {'year': '2023', 'GWL': 1.31}}\n",
    "observations = {'WMO': {'yr': 1.44}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute year 2024\n",
    "\"\"\"\n",
    "targets = {'yr':  {'year': '2024', 'GWL': 1.36}}\n",
    "observations = {'WMO': {'yr': 1.52}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic routines to prepare for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporality = set()\n",
    "for _obs in observations:\n",
    "    temporality.update(list(observations[_obs].keys()))\n",
    "temporality = list(temporality)\n",
    "int_low = (1. - confidence_range) / 2.; int_high = 1. - int_low\n",
    "temporality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allModels = ['ACCESS-ESM1-5', 'CNRM-CM6-1', 'CanESM5', 'IPSL-CM6A-LR', 'MIROC6', 'MPI-ESM1-2-LR']\n",
    "allSSPs = ['hist-ssp126', 'hist-ssp245', 'hist-ssp370', 'hist-ssp585']\n",
    "\n",
    "saveName = variable\n",
    "saveName += '_tempo'\n",
    "saveName += '_ref'+piStr+piEnd\n",
    "saveName += '_win'+str(window_size)\n",
    "if isinstance(model, list) and set(model).intersection(set(allModels)) == set(allModels):\n",
    "    saveName += '_CMIP6MS'\n",
    "    ensemble_lgd = 'CMIP6-MS'\n",
    "    ensemble_size_min = 1\n",
    "    model = allModels\n",
    "elif model is None:\n",
    "    saveName += '_allCMIP6Models'\n",
    "    if ensemble_size_min != 1:\n",
    "        saveName += str(ensemble_size_min)\n",
    "    ensemble_lgd = 'CMIP6'\n",
    "    model = None\n",
    "if only_one_member:\n",
    "    saveName += '_1mem'\n",
    "if experiment is None or set(experiment).intersection(set(allSSPs)) == set(allSSPs):\n",
    "    saveName += '_allSSP'\n",
    "else:\n",
    "    saveName += '_'+'-'.join(experiment).replace('hist-', '').replace('-ssp', '-')\n",
    "\n",
    "saveName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _tempo in temporality:\n",
    "    _tmp = outDir+_tempo.join(saveName.split('tempo'))\n",
    "    if not os.path.exists(_tmp):\n",
    "        os.makedirs(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict = {\n",
    "    'tas': 'GSAT',\n",
    "    'pr': 'precipitations',\n",
    "    'nasst': 'North Atlantic SST',\n",
    "    'natas': 'North Atlantic SAT',\n",
    "    'naesst': 'North East Atlantic SST',\n",
    "    'naetas': 'North East Atlantic SAT',\n",
    "    'naessst': 'Horseshoe SST',\n",
    "    'naestas': 'North East Atlantic SAT',\n",
    "    'naecsst': 'North West Atlantic SST',\n",
    "    'naectas': 'North West Atlantic SAT',\n",
    "    'naescsst': 'Complement HS SST',\n",
    "    'naesctas': 'North West Atlantic SAT',\n",
    "}\n",
    "\n",
    "temporality_dict = {\n",
    "    'yr': 'Annual',\n",
    "    'DJF': 'hivernale (DJF)',\n",
    "    'JFM': 'hivernale (JFM)',\n",
    "    'JJA': 'estivale (JJA)',\n",
    "    'MJJ': 'May-June-July',\n",
    "    'MJ': 'May-June',\n",
    "    'm01': 'January',\n",
    "    'm04': \"d'avril\",\n",
    "    'm05': 'May',\n",
    "    'm06': 'June',\n",
    "    'm09': 'September',\n",
    "}\n",
    "\n",
    "lgd_obs_dict = {\n",
    "    'Berkeley Earth': 'BEST',\n",
    "    'ERA5': 'ERA5',\n",
    "    'NOAAGlobalTempv6': 'NOAA',\n",
    "    'WMO': 'Consolidated obs.',\n",
    "}\n",
    "\n",
    "driver_dict = {\n",
    "    'amv':      {'variable': 'natas',   'tempo': 'yr',  'lag': 0},\n",
    "    'nino34':   {'variable': 'nino34',  'tempo': 'OND', 'lag': 1},\n",
    "}\n",
    "driver_dict['amvGlob'] = driver_dict['amv']\n",
    "driver_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_lvls = [targets[_tempo]['GWL'] for _tempo in temporality]\n",
    "gwl_lvls = list(set(gwl_lvls))\n",
    "gwl_lvls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dict = {'historical': {'name': 'Historical', 'color': 'gray'},\n",
    "                   'piControl': {'name': 'Pre-industrical Control', 'color': 'black'},\n",
    "                   'ssp119': {'name': 'SSP1-1.9', 'color': 'green'},\n",
    "                   'ssp126': {'name': 'SSP1-2.6', 'color': 'blue'},\n",
    "                   'ssp245': {'name': 'SSP2-4.5', 'color': 'darkgoldenrod'},\n",
    "                   'ssp370': {'name': 'SSP3-7.0', 'color': 'red'},\n",
    "                   'ssp434': {'name': 'SSP4.3-4', 'color': 'purple'},\n",
    "                   'ssp585': {'name': 'SSP5-8.5', 'color': 'brown'}}\n",
    "\n",
    "\n",
    "experiment_dict['hist-ssp119'] = experiment_dict['ssp119']\n",
    "experiment_dict['hist-ssp126'] = experiment_dict['ssp126']\n",
    "experiment_dict['hist-ssp245'] = experiment_dict['ssp245']\n",
    "experiment_dict['hist-ssp370'] = experiment_dict['ssp370']\n",
    "experiment_dict['hist-ssp434'] = experiment_dict['ssp434']\n",
    "experiment_dict['hist-ssp585'] = experiment_dict['ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_IV_ensemble(members:list=None):\n",
    "    _dict = {}\n",
    "    for _member in members:\n",
    "        _source, _experiment, _ripf = _member.split('_')\n",
    "        _config = 'r*i'+_ripf.split('i')[1]\n",
    "        if _source not in _dict:\n",
    "            _dict[_source] = {_experiment: {_config: [_member]}}\n",
    "        elif _experiment not in _dict[_source]:\n",
    "            _dict[_source][_experiment] = {_config: [_member]}\n",
    "        elif _config not in _dict[_source][_experiment]:\n",
    "            _dict[_source][_experiment][_config] = [_member]\n",
    "        else:\n",
    "            _dict[_source][_experiment][_config].append(_member)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season2m_dict = {'DJ':1,'JF':2,'FM':3,'MA':4,'AM':5,'MJ':6,'JJ':7,'JA':8,'AS':9,'SO':10,'ON':11,'ND':12}\n",
    "season3m_dict = {'DJF':1,'JFM':2,'FMA':3,'MAM':4,'AMJ':5,'MJJ':6,'JJA':7,'JAS':8,'ASO':9,'SON':10,'OND':11,'NDJ':12}\n",
    "season4m_dict = {'ONDJ':1,'NDJF':2,'DJFM':3,'JFMA':4,'FMAM':5,'MAMJ':6,'AMJJ':7,'MJJA':8,'JJAS':9,'JASO':10,'ASON':11,'SOND':12}\n",
    "season5m_dict = {'NDJFM':1,'DJFMA':2,'JFMAM':3,'FMAMJ':4,'MAMJJ':5,'AMJJA':6,'MJJAS':7,'JJASO':8,'JASON':9,'ASOND':10,'SONDJ':11,'ONDJF':12}\n",
    "season6m_dict = {'ASONDJ':1,'SONDJF':2,'ONDJFM':3,'NDJFMA':4,'DJFMAM':5,'JFMAMJ':6,'FMAMJJ':7,'MAMJJA':8,'AMJJAS':9,'MJJASO':10,'JJASON':11,'JASOND':12}\n",
    "season12m_dict = {'FMAMJJASONDJ':1,'MAMJJASONDJF':2,'AMJJASONDJFM':3,'MJJASONDJFMA':4,'JJASONDJFMAM':5,'JASONDJFMAMJ':6,'ASONDJFMAMJJ':7,'SONDJFMAMJJA':8,'ONDJFMAMJJAS':9,'NDJFMAMJJASO':10,'DJFMAMJJASON':11,'JFMAMJJASOND':12}\n",
    "\n",
    "def get_season(data, season):\n",
    "    if season == 'yr':\n",
    "        return data.resample(time='YS').mean(dim='time', keep_attrs = True, skipna=False)\n",
    "    elif len(season) <= 3 and season[-1] == 'm':\n",
    "        return data.rolling(time = int(season[:-1]), center = True).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "    elif len(season) <= 3 and season[0] == 'm':\n",
    "        return data.groupby('time.month')[int(season[1:])]\n",
    "    elif season in season2m_dict.keys():\n",
    "        _seasons = data.rolling(time = 2, center = False).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season2m_dict[season]))\n",
    "    elif season in season3m_dict.keys():\n",
    "        _seasons = data.rolling(time = 3, center = True).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season3m_dict[season]))\n",
    "    elif season in season4m_dict.keys():\n",
    "        _seasons = data.rolling(time = 4, center = False).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season4m_dict[season]))\n",
    "    elif season in season5m_dict.keys():\n",
    "        _seasons = data.rolling(time = 5, center = True).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season5m_dict[season]))\n",
    "    elif season in season6m_dict.keys():\n",
    "        _seasons = data.rolling(time = 6, center = False).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season6m_dict[season]))\n",
    "    elif season in season12m_dict.keys():\n",
    "        _seasons = data.rolling(time = 12, center = False).mean(dim='time', keep_attrs = True, skipna=False)\n",
    "        return _seasons.sel(time = np.in1d(_seasons['time.month'], season12m_dict[season]))\n",
    "    elif isinstance(season, int):\n",
    "        try:\n",
    "            return data.groupby('time.month')[season]\n",
    "        except:\n",
    "            print('Error; Nothing happened')\n",
    "            return data\n",
    "    else:\n",
    "        print('Error; Nothing happened')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(data:xr, scaler='std', period:tuple=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array of either DataArray or Dataset type from the xarray package\n",
    "    scaler: can either be a string, a float, or an int\n",
    "     - 'std': divided by the standard deviation\n",
    "     - float in (0., 1.] : devided by the interquantile range\n",
    "     - int in (0., 100.] : devided by the interpercentile range\n",
    "    period: None or tuple (or list)\n",
    "\n",
    "    ! Add an option to put back the forced response?\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: array of same type as data, normalised according to the given period\n",
    "    '''\n",
    "    if isinstance(data, xr.Dataset):\n",
    "        _das = [data.get(_var) for _var in data.data_vars]; _to_ds = True;\n",
    "    elif isinstance(data, xr.DataArray):\n",
    "        _das = [data]; _to_ds = False;\n",
    "    else:\n",
    "        raise TypeError('Input data type should either be a Dataset or a DataArray from the xarray module.')\n",
    "    _start = _stop = None\n",
    "    if isinstance(period, tuple) or isinstance(period, list):\n",
    "        if len(period) == 2:\n",
    "            _start, _stop = period\n",
    "    _dict = sort_by_IV_ensemble(list(data.member.values))\n",
    "    _out = list()\n",
    "    for _da in _das:\n",
    "        if 'member' in _da.dims:\n",
    "            _dims = ['member']\n",
    "            if 'time' in _da.dims:\n",
    "                _dims.append('time')\n",
    "            _store = list()\n",
    "            for s in _dict:\n",
    "                for e in _dict[s]:\n",
    "                    for c in _dict[s][e]:\n",
    "                        _tmp = _da.sel(member=_dict[s][e][c])\n",
    "                        _tmp_ref = _tmp.sel(time=slice(_start, _stop)) if 'time' in _tmp.dims and period is not None else _tmp\n",
    "                        if isinstance(scaler, int):\n",
    "                            if scaler > 0 and scaler <= 100:\n",
    "                                scaler /= 100.\n",
    "                            else:\n",
    "                                scaler = 'std'; print('ERROR: if scaler is of type int, it must be in (0, 100].\\nStandard deviation used instead.')\n",
    "                        elif isinstance(scaler, float):\n",
    "                            if scaler > 0. and scaler <= 1.:\n",
    "                                _scl = _tmp_ref.quantile(1.-(1.-scaler)/2., dim=_dims) - _tmp_ref.quantile((1.-scaler)/2., dim=_dims)\n",
    "                            else:\n",
    "                                scaler = 'std'; print('ERROR: if scaler is of type float, it must be in (0., 1.].\\nStandard deviation used instead.')\n",
    "                        elif scaler is None:\n",
    "                            _scl = 1.\n",
    "                        else:\n",
    "                            scaler = 'std'; print('ERROR: scaler not understood.\\nStandard deviation used instead.')\n",
    "                        if scaler == 'std':\n",
    "                            _scl = _tmp_ref.std(dim=_dims, keep_attrs=True)\n",
    "                        _tmp = (_tmp - _tmp_ref.mean(dim=_dims, keep_attrs=True)) / _scl\n",
    "                        try:\n",
    "                            if _scl.size != 1:\n",
    "                                _tmp = xr.where(_scl < 1e-10, 0., _tmp)\n",
    "                        except:\n",
    "                            pass\n",
    "                        _store.append(_tmp)\n",
    "            _out.append(xr.concat(_store, dim='member'))\n",
    "    if _to_ds:\n",
    "        out = xr.merge(_out)\n",
    "        out.attrs = data.attrs\n",
    "    else:\n",
    "        out = _out[0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(members, method='model', dim='member'):\n",
    "    _weight = list()\n",
    "    _members = list()\n",
    "    if isinstance(members, xr.DataArray) or isinstance(members, xr.Dataset):\n",
    "        members = members[dim].values\n",
    "    _dict = sort_by_IV_ensemble(members)\n",
    "    for _source in _dict:\n",
    "        for _experiment in _dict[_source]:\n",
    "            for _config in _dict[_source][_experiment]:\n",
    "                _members += _dict[_source][_experiment][_config]\n",
    "                [_weight.append(\n",
    "                    1. / (\n",
    "                          len(_dict[_source])\n",
    "                        * len(_dict[_source][_experiment])\n",
    "                        * len(_dict[_source][_experiment][_config])\n",
    "                    )\n",
    "                ) for _ in range(len(_dict[_source][_experiment][_config]))]\n",
    "    out = xr.DataArray(\n",
    "        _weight,\n",
    "        dims = [dim],\n",
    "        coords = {dim: _members}\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_estimated_forcing(data:xr.DataArray=None, min_size=1):\n",
    "    '''\n",
    "    Make it xarray.Dataset fiendly!\n",
    "    '''\n",
    "    _dict = sort_by_IV_ensemble(list(data.member.values))\n",
    "    _tmp = list()\n",
    "    for s in _dict:\n",
    "        for e in _dict[s]:\n",
    "            for c in _dict[s][e]:\n",
    "                if len(_dict[s][e][c]) >= min_size:\n",
    "                    _mean = data.sel(member=_dict[s][e][c]).mean(dim='member', keep_attrs=True)\n",
    "                    for l in _dict[s][e][c]:\n",
    "                        _tmp.append(_mean.assign_coords({'member':l}))\n",
    "                        _tmp[-1].attrs=data.attrs\n",
    "                else:\n",
    "                    _tmp.append(xr.full_like(data.sel(member=_dict[s][e][c]), np.nan))\n",
    "\n",
    "    return xr.concat(_tmp, dim='member')\n",
    "\n",
    "\n",
    "def remove_forced_response(data:xr.DataArray=None, min_size=1):\n",
    "    '''\n",
    "    Make it xarray.Dataset fiendly!\n",
    "    '''\n",
    "    return data - ensemble_estimated_forcing(data, min_size=min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def get_trend(data, dim='time', groupby='time.month', **kwargs):\n",
    "    '''\n",
    "    Computes the linear trend on a given dimension.\n",
    "    '''\n",
    "    if isinstance(data, xr.Dataset):\n",
    "        _das = [data.get(_var) for _var in data.data_vars]; _to_ds = True;\n",
    "    elif isinstance(data, xr.DataArray):\n",
    "        _das = [data]; _to_ds = False;\n",
    "    else:\n",
    "        raise TypeError('Input data type should either be a Dataset or a DataArray from the xarray module.')\n",
    "\n",
    "    _trend = []\n",
    "    for _da in _das:\n",
    "        if dim in _da.dims and dim not in _da.name:\n",
    "            _tmp_da = _da.transpose(..., dim)\n",
    "            if dim == 'time' and groupby is not None:\n",
    "                _da_trend = xr.concat(\n",
    "                    [titi[1] - xr.DataArray(name = titi[1].name,\n",
    "                        data = scipy.signal.detrend(titi[1], **kwargs),\n",
    "                        dims = titi[1].dims,\n",
    "                        coords = titi[1].coords) for titi in _tmp_da.groupby(groupby)], dim='time')\n",
    "            else:\n",
    "                _da_trend = _tmp_da - xr.DataArray(name = _tmp_da.name,\n",
    "                    data = scipy.signal.detrend(_tmp_da, **kwargs),\n",
    "                    dims = _tmp_da.dims,\n",
    "                    coords = _tmp_da.coords)\n",
    "            for _dim in _da.dims:\n",
    "                _da_trend = _da_trend.transpose(..., _dim)\n",
    "            _da_trend = _da_trend.reindex_like(_da)\n",
    "            _da_trend.attrs = _da.attrs\n",
    "        _trend.append(_da_trend)\n",
    "\n",
    "    if _to_ds:\n",
    "        out = xr.merge(_trend)\n",
    "        out.attrs = data.attrs\n",
    "    else:\n",
    "        out = _trend[0]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_weights(da, method = 'member-per-model'):\n",
    "    _members = xr.where(da, 1., np.nan).dropna(dim='member', how='all').member\n",
    "    _weights = get_weight(_members, method='model').reindex_like(da, fill_value=0.)\n",
    "    if method in ['mpm', 'member-per-model']:\n",
    "        pass\n",
    "    elif method in ['opm', 'occurrence-per-model']:\n",
    "        _weights = (_weights/da.sum(dim='time')).fillna(0.)\n",
    "    else:\n",
    "        raise OptionNotCoddedYet\n",
    "    return _weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bootstrap(data, weight, n_boot=1000, dim='member'):\n",
    "    _coords = data[dim]\n",
    "    _data = data.rename({dim: 'sample'})\n",
    "    _tmp_boot = list()\n",
    "    for i in range(n_boot):\n",
    "        _tmp = _data.sel(sample=np.random.choice(_data.sample, size=len(_data.sample), replace=True, p=weight/weight.sum()))\n",
    "        _tmp_boot.append(_tmp.assign_coords({'boot': i+1}).drop_vars('sample'))\n",
    "    return xr.concat(_tmp_boot, dim='boot').rename({'sample': dim}).assign_coords({dim: _coords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_period(data, weight, event, bootstrap = False, data_boot = None, low = .05, high = .95):\n",
    "\n",
    "    _mean = data.weighted(weight).mean()\n",
    "    _median = data.weighted(weight).quantile(.5)\n",
    "    _std = data.weighted(weight).std()\n",
    "    _skew = ((data ** 3.).weighted(weight).mean() - 3. * _mean * _std ** 2. - _mean ** 3. ) / (_std ** 3.)\n",
    "\n",
    "\n",
    "    data = data.sortby(data, ascending = True if event > _mean else False)\n",
    "\n",
    "    _index = int(abs(data-event).argmin().values)\n",
    "    if _index < 1e-5*len(data) or data[_index] == data.min():\n",
    "        _p = 0. ; _computed = np.nan\n",
    "    elif _index > (1-1e-5)*len(data) or data[_index] == data.max():\n",
    "        _p = 1. ; _computed = np.nan\n",
    "    else:\n",
    "        _p = (weight.isel(sample=slice(None, _index+1)).sum()/weight.sum()).values ; _computed = int(round(1. / (1. - _p), 0))\n",
    "\n",
    "    _p = stats.norm.cdf(observations[_obs][_tempo], loc=_mean, scale=_std)\n",
    "    _normal = int(round(1. / (1. - _p) if event > _mean else 1. / _p, 0)) if _p not in [0., 1.] else np.nan\n",
    "    _p = stats.skewnorm.cdf(observations[_obs][_tempo], _skew, loc=_mean, scale=_std)\n",
    "    _skew_normal = int(round(1. / (1. - _p) if event > _mean else 1. / _p, 0)) if _p not in [0., 1.] else np.nan\n",
    "\n",
    "    if bootstrap:\n",
    "        _p_boot = list()\n",
    "        for _boot in data_boot.boot:\n",
    "            _da_boot = data_boot.sel(boot=_boot).dropna('sample', how='all')\n",
    "            _da_boot = _da_boot.sortby(_da_boot, ascending = True if event > _mean else False)\n",
    "            _index = int(abs(_da_boot-event).argmin().values)\n",
    "            if _index < 1e-5*len(_da_boot) or _da_boot[_index] == _da_boot.min():\n",
    "                _p_boot.append(0.)\n",
    "            elif _index > (1-1e-5)*len(_da_boot) or _da_boot[_index] == _da_boot.max():\n",
    "                _p_boot.append(1.)\n",
    "            else:\n",
    "                _p_boot.append((_index+1)/len(_da_boot))\n",
    "        try:\n",
    "            _bootstrap_low = int(round(1. / (1. - np.quantile(_p_boot, low)), 0))\n",
    "        except:\n",
    "            _bootstrap_low = np.nan\n",
    "        try:\n",
    "            _bootstrap_med = int(round(1. / (1. - np.quantile(_p_boot, .5)), 0))\n",
    "        except:\n",
    "            _bootstrap_med = np.nan\n",
    "        try:\n",
    "            _bootstrap_high = int(round(1. / (1. - np.quantile(_p_boot, high)), 0))\n",
    "        except:\n",
    "            _bootstrap_high = np.nan\n",
    "    else:\n",
    "        _bootstrap_low = _bootstrap_med = _bootstrap_high = np.nan\n",
    "\n",
    "    return [_computed, _normal, _skew_normal, _bootstrap_low, _bootstrap_med, _bootstrap_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_global_warming_levels(da, window, levels=[1.5, 2., 3., 4.], method='cross', interval=0.1):\n",
    "    \"\"\"\n",
    "    da: DataArray containing global average temperature with a 'time' dimension.\n",
    "    window: Number of years (N) on which a time average is applied in order to compute the moving global average temperature.\n",
    "    levels: List of target levels.\n",
    "    method:\n",
    "     - 'cross': returns the N years for which the global average temperature first crossed the dedicated warming level,\n",
    "     - 'state': returns all the sets of N years for which the global average temperature was near the dedicated level (interval/2).\n",
    "    interval: Interval used around each level if the method is 'state'.\n",
    "    \"\"\"\n",
    "    _rolled = da.rolling(time=window, center=True, min_periods=window).mean(skipna=False)\n",
    "    _lst = list()\n",
    "    if method == 'cross':\n",
    "        for _level in levels:\n",
    "            _tmp = xr.where(_rolled >= _level, 1., 0.).rolling(time=window, center=True, min_periods=window).mean(skipna=False)\n",
    "            _tmp = _tmp.where(_tmp != 0.)\n",
    "            _tmp += np.arange(len(_tmp.time))\n",
    "            _lst.append(xr.where(_tmp == _tmp.min(dim='time'), 1., 0.).rolling(time=window, center=False, min_periods=1).mean(skipna=False))\n",
    "    elif method == 'state':\n",
    "        [_lst.append(xr.where((_rolled > _level-interval/2.) & (_rolled < _level+interval/2.), 1., 0.).rolling(time=window, center=True, min_periods=1).mean()) for _level in levels]\n",
    "    return xr.where(xr.concat(_lst, pd.Index(levels, name='warming_level')) != 0., True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data_dict, stats_str, stats_end, window_size, center=True):\n",
    "    _std_list = list() ; _qdw_list = list() ; _qup_list = list()\n",
    "    for _var in data_dict:\n",
    "        _data = xr.open_dataset(data_dict[_var]['path']).get(data_dict[_var]['var'])\n",
    "        _data_yr = _data.sel(time=slice(stats_str, stats_end)).resample(time='YS').mean(dim = 'time', keep_attrs = True)\n",
    "        _data_yr = _data_yr - _data_yr.rolling(time=window_size, center=center).mean()\n",
    "        _std_list.append(float(_data_yr.std().values))\n",
    "        _qdw_list.append(float(_data_yr.quantile(.05).values))\n",
    "        _qup_list.append(float(_data_yr.quantile(.95).values))\n",
    "    _ds_stats_out = xr.Dataset(\n",
    "        data_vars={\n",
    "            'std': (['source'], _std_list),\n",
    "            'qdw': (['source'], _qdw_list),\n",
    "            'qup': (['source'], _qup_list),\n",
    "        },\n",
    "        coords = {'source': list(obs_dict.keys())}\n",
    "    )\n",
    "    _ds_stats_out = _ds_stats_out.sortby(_ds_stats_out['std'])\n",
    "    return _ds_stats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(x, std, qdw, qup, ax, color):\n",
    "    ax[0].plot(x, std, 'o', mfc='w', mec=color, ms=7)\n",
    "    ax[0].axhline(y=std, ls='--', lw=.75, c=color, zorder=0)\n",
    "    ax[1].plot(x, qdw, 'v', mfc='w', mec=color, ms=7)\n",
    "    ax[1].axhline(y=qdw, ls='--', lw=.75, c=color, zorder=0)\n",
    "    ax[1].plot(x, qup, '^', mfc='w', mec=color, ms=7)\n",
    "    ax[1].axhline(y=qup, ls='--', lw=.75, c=color, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "\n",
    "def mean(data, weights=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array_like, 1-D or 2-D\n",
    "        dataset\n",
    "\n",
    "    weights: None or 1-D ndarray\n",
    "        weights for each observation, with same length as zero axis of data\n",
    "    '''\n",
    "    return DescrStatsW(data, weights=weights).mean\n",
    "\n",
    "\n",
    "def var(data, weights=None):\n",
    "    return DescrStatsW(data, weights=weights).var\n",
    "\n",
    "\n",
    "def std(data, weights=None):\n",
    "    return DescrStatsW(data, weights=weights).std\n",
    "\n",
    "\n",
    "def median(data, weights=None):\n",
    "    return DescrStatsW(data, weights=weights).quantile(0.5)\n",
    "\n",
    "\n",
    "def quantile(data, probs, weights=None, return_pandas=False):\n",
    "    return DescrStatsW(data, weights=weights).quantile(probs, return_pandas=return_pandas)\n",
    "\n",
    "\n",
    "def cov(data, weights=None):\n",
    "    return DescrStatsW(data, weights=weights).cov\n",
    "\n",
    "\n",
    "def demeaned(data, weights=None):\n",
    "    return DescrStatsW(data, weights=weights).demeaned\n",
    "\n",
    "\n",
    "def ttest_mean(data, value=0, alternative='two-sided', weights=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    value: float or array\n",
    "        The hypothesized value for the mean (0 by default).\n",
    "\n",
    "    alternative: str\n",
    "        The alternative hypothesis, H1, has to be one of the following:\n",
    "         - 'two-sided': H1: mean not equal to value (default)\n",
    "         - 'larger' : H1: mean larger than value\n",
    "         - 'smaller' : H1: mean smaller than value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    tstat: float\n",
    "        Test statistic\n",
    "\n",
    "    pvalue: float\n",
    "        pvalue of the t-test\n",
    "\n",
    "    df: int or float\n",
    "    '''\n",
    "    return DescrStatsW(data, weights=weights).ttest_mean(value=value, alternative=alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(data, ax = None,\n",
    "            weights=None,\n",
    "            label = None,\n",
    "            yTitle = None,\n",
    "            bar = 'median', box = .5, ext = .1,\n",
    "            alpha = 1,\n",
    "            color = None,\n",
    "            edgecolor = None,\n",
    "            hatch = None,\n",
    "            outliers = True,\n",
    "            extrema = False,\n",
    "            markersize = 3,\n",
    "            dx=0, width=0.5,\n",
    "            rotation=0,\n",
    "            orientation='vertical'):\n",
    "    \"\"\"\n",
    "    Create a boxplot of the dispersion of *data*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    bar : 'mean' or 'median' or 'both'\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    _ax = ax or plt.gca()\n",
    "    _size = np.shape(data)\n",
    "\n",
    "    _color = color or ['black' for i in range (0, _size[0])]\n",
    "    _edgecolor = edgecolor or ['black' for i in range (0, _size[0])]\n",
    "    _hatch = hatch or [None for i in range (0, _size[0])]\n",
    "    _label = label or ['' for i in range (0, _size[0])]\n",
    "    for i in range(0, _size[0]):\n",
    "        _tmp = np.array(data[i])\n",
    "        _weights = weights[i] if weights is not None else weights\n",
    "        if len(np.shape(_tmp)) > 1:\n",
    "            _tmp = np.concatenate(data[i])\n",
    "        if len(np.shape(_weights)) > 1:\n",
    "            _weights = np.concatenate(_weights[i]) if weights is not None else None\n",
    "        if len(_tmp) != 1:\n",
    "            # compute statistics\n",
    "            _median = median(_tmp, weights=_weights)\n",
    "            _mean = mean(_tmp, weights=_weights)\n",
    "            _boxlow = quantile(_tmp, box/2, weights=_weights)\n",
    "            _boxhigh = quantile(_tmp, 1 - box/2, weights=_weights)\n",
    "            _low = quantile(_tmp, ext/2, weights=_weights)\n",
    "            _high = quantile(_tmp, 1 - ext/2, weights=_weights)\n",
    "            _minima = np.min(_tmp)\n",
    "            _maxima = np.max(_tmp)\n",
    "            # plot\n",
    "            _fill = True if _hatch[i] is None else None\n",
    "            _x_data = [\n",
    "                (dx + i, dx + i),\n",
    "                (dx + i, dx + i),\n",
    "                (dx + i - width / 4., dx + i + width / 4.),\n",
    "                (dx + i - width / 4., dx + i + width / 4.),\n",
    "                (dx + i - width / 2., dx + i + width / 2.),\n",
    "                (dx + i - width / 2., dx + i + width / 2.),\n",
    "                i,\n",
    "                i,\n",
    "            ]\n",
    "            _y_data = [\n",
    "                (_low, _boxlow),\n",
    "                (_boxhigh, _high),\n",
    "                (_low, _low),\n",
    "                (_high, _high),\n",
    "                (_mean, _mean),\n",
    "                (_median, _median),\n",
    "                _minima,\n",
    "                _maxima,\n",
    "            ]\n",
    "            if orientation == 'vertical':\n",
    "                _X = _x_data; _Y = _y_data\n",
    "                _ax.add_patch(patches.Rectangle(\n",
    "                    (dx + i - width / 2., _boxlow), width, _boxhigh - _boxlow,\n",
    "                    facecolor = _color[i], edgecolor = _edgecolor[i], hatch = _hatch[i], fill = _fill, alpha = alpha))\n",
    "            elif orientation == 'horizontal':\n",
    "                _X = _y_data; _Y = _x_data\n",
    "                _ax.add_patch(patches.Rectangle(\n",
    "                    (_boxlow, dx + i - width / 2.), _boxhigh - _boxlow, width,\n",
    "                    facecolor = _color[i], edgecolor = _edgecolor[i], hatch = _hatch[i], fill = _fill, alpha = alpha))\n",
    "\n",
    "            _ax.plot(_X[0], _Y[0], 'k', ls = '--', lw = 1.5)\n",
    "            _ax.plot(_X[1], _Y[1], 'k',ls = '--', lw = 1.5)\n",
    "            _ax.plot(_X[2], _Y[2], 'k', lw = 1.5)\n",
    "            _ax.plot(_X[3], _Y[3], 'k', lw = 1.5)\n",
    "            \n",
    "            if bar == 'mean':\n",
    "                _ax.plot(_X[4], _Y[4], color = _color[i], lw = 7)\n",
    "                _ax.plot(_X[4], _Y[4], color = 'w', lw = 2)\n",
    "            elif bar == 'median':\n",
    "                _ax.plot(_X[5], _Y[5], color = _color[i], lw = 7)\n",
    "                _ax.plot((dx + i - width / 2., dx + i + width / 2.), _Y[5], color = 'w', lw = 2)\n",
    "            elif bar == 'both':\n",
    "                _ax.plot(_X[4], _Y[4], color = 'k', lw = 7)\n",
    "                _ax.plot(_X[5], _Y[5], color = _color[i], lw = 7)\n",
    "                _ax.plot(_X[5], _Y[5], color = 'w', lw = 2)\n",
    "\n",
    "            if outliers:\n",
    "                for j in range(0, len(_tmp)):\n",
    "                    if (_tmp[j] < _low or _tmp[j] > _high):\n",
    "                        if orientation == 'vertical':\n",
    "                            _ax.plot(dx + i, _tmp[j], 'o', c = 'k', markersize = markersize)\n",
    "                        elif orientation == 'horizontal':\n",
    "                            _ax.plot(_tmp[j], dx + i, 'o', c = 'k', markersize = markersize)\n",
    "            if extrema:\n",
    "                _ax.plot(_X[6], _Y[6], 'o', c=_color[i], markersize=markersize)\n",
    "                _ax.plot(_X[7], _Y[7], 'o', c=_color[i], markersize=markersize)\n",
    "\n",
    "    if orientation == 'vertical':\n",
    "        _ax.set_xticks(np.arange(_size[0]))\n",
    "        _ax.set_xticklabels(_label, rotation=rotation)\n",
    "        _ax.set_ylabel(yTitle)\n",
    "    elif orientation == 'horizontal':\n",
    "        _ax.set_yticks(np.arange(_size[0]))\n",
    "        _ax.set_yticklabels(_label, rotation=rotation)\n",
    "        _ax.set_xlabel(yTitle)\n",
    "\n",
    "    return _ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_averages(x, y, z, zmin, zmax):\n",
    "    _sup_mask = xr.where(z > zmin, True, False)\n",
    "    _inf_mask = xr.where(z < zmax, True, False)\n",
    "    _z = z.where(_sup_mask).where(_inf_mask).stack(xy=('member', 'time')).dropna(dim='xy')\n",
    "    _x = x.stack(xy=('member', 'time')).sel(xy=_z.xy).unstack() ; _y = y.stack(xy=('member', 'time')).sel(xy=_z.xy).unstack()\n",
    "    _w = local_weights(_z.unstack(), method='occurrence-per-model')\n",
    "    return _x.weighted(_w).mean(), _y.weighted(_w).mean(), _z.unstack().weighted(_w).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "def get_diverging_cmap(vmin = -1., vmax = 1., origin = 0., neg = 'Blues', pos = 'YlOrBr'):\n",
    "    viridis = matplotlib.cm.get_cmap('viridis', 256)\n",
    "    newcolors = viridis(np.linspace(0, 1, 256))\n",
    "    if vmin < origin and vmax > origin:\n",
    "        vrange = vmax - vmin\n",
    "        zero = int((origin-vmin) / vrange * 256)\n",
    "        negcolors = matplotlib.cm.get_cmap(neg, zero)\n",
    "        poscolors = matplotlib.cm.get_cmap(pos, 256 - zero)\n",
    "        newcolors[:zero, :] = negcolors(np.linspace(1, 0, zero))\n",
    "        newcolors[zero:, :] = poscolors(np.linspace(0, 1, 256 - zero))\n",
    "        return matplotlib.colors.ListedColormap(newcolors)\n",
    "    elif vmin >= origin and vmax > vmin:\n",
    "        zero = int((vmin-origin) / (vmax-origin) * 256)\n",
    "        poscolors = matplotlib.cm.get_cmap(pos, 256-zero)\n",
    "        newcolors = poscolors(np.linspace((vmin-origin)/(vmax-origin), 1, 256-zero))\n",
    "        return matplotlib.colors.ListedColormap(newcolors)\n",
    "    elif vmax <= origin and vmax > vmin:\n",
    "        zero = int((vmax-vmin) / (origin-vmin) * 256)\n",
    "        negcolors = matplotlib.cm.get_cmap(neg, zero)\n",
    "        newcolors = negcolors(np.linspace(1, (vmax-vmin) / (origin-vmin), zero))\n",
    "        return matplotlib.colors.ListedColormap(newcolors)\n",
    "    else:\n",
    "        print('ERROR in get_cmap().')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes about 1'30''\n",
    "list_of_simulations = glob.glob(\n",
    "    dataDir+'Simulations/tas_Amon_*_hist-*_global_*.nc'\n",
    ")\n",
    "list_of_simulations.sort()\n",
    "data_list = list()\n",
    "for _in_file in list_of_simulations:\n",
    "    _load = True\n",
    "    if model is not None:\n",
    "        if _in_file.split('_')[2] not in model:\n",
    "            _load = False\n",
    "    if experiment is not None:\n",
    "        if _in_file.split('_')[3] not in experiment:\n",
    "            _load = False\n",
    "    if _load:\n",
    "        _in_ds = xr.open_dataset(_in_file).get('tas')\n",
    "        if int(_in_ds.time.dt.year[-1]) >= year_min:\n",
    "            if 'member' not in _in_ds.dims:\n",
    "                _in_ds = _in_ds.assign_coords({'member': '_'.join(_in_file.split('_')[2:5])})\n",
    "            data_list.append(_in_ds)\n",
    "da_tas_glo = xr.concat(data_list, dim='member').dropna(dim='member',how='all')\n",
    "da_tas_glo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading regional variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes about 1'30'' if variable is not 'tas'\n",
    "if variable == 'tas':\n",
    "    da_region = da_tas_glo\n",
    "else:\n",
    "    if variable in ('nasst', 'natas', 'naesst', 'naetas', 'naessst', 'naecsst', 'naescsst'):\n",
    "        list_of_simulations = glob.glob(\n",
    "            dataDir+'Simulations/'+variable+'_mon_*_hist-*_index_*.nc'\n",
    "        )\n",
    "    else:\n",
    "        list_of_simulations = glob.glob(\n",
    "            dataDir+'Simulations/'+variable+'_Amon_*_hist-*_index_*.nc'\n",
    "        )\n",
    "    list_of_simulations.sort()\n",
    "    data_list = list()\n",
    "    for _in_file in list_of_simulations:\n",
    "        _load = True\n",
    "        if model is not None:\n",
    "            if _in_file.split('_')[2] not in model:\n",
    "                _load = False\n",
    "        if experiment is not None:\n",
    "            if _in_file.split('_')[3] not in experiment:\n",
    "                _load = False\n",
    "        if only_one_member:\n",
    "            if 'r1i' not in str(_in_file.split('_')[4]):\n",
    "                _load = False\n",
    "        if _load:\n",
    "            _in_ds = xr.open_dataset(_in_file).get(variable)\n",
    "            if int(_in_ds.time.dt.year[-1]) >= year_min:\n",
    "                _in_ds['time'] = _in_ds['time'].astype('datetime64[ns]')\n",
    "                if 'member' not in _in_ds.dims:\n",
    "                    _in_ds = _in_ds.assign_coords({'member': '_'.join(_in_file.split('_')[2:5])})\n",
    "                try:\n",
    "                    _in_ds = _in_ds.drop_vars('height')\n",
    "                except:\n",
    "                    pass\n",
    "                data_list.append(_in_ds)\n",
    "    da_region = xr.concat(data_list, dim='member').dropna(dim='member', how='all')\n",
    "    da_region = da_region.resample(time='M').mean(dim = 'time', keep_attrs = True)\n",
    "da_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining largest set of common members for regional and global simulations, and merging of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_members_full = list(\n",
    "    set(da_tas_glo.member.values).intersection(set(da_region.member.values))\n",
    ")\n",
    "common_members = list()\n",
    "if experiment is not None:\n",
    "    for _member in common_members_full:\n",
    "        if _member.split('_')[1] in experiment:\n",
    "            common_members.append(_member)\n",
    "else:\n",
    "    common_members = common_members_full\n",
    "common_members.sort()\n",
    "da_tas_glo = da_tas_glo.sel(member=common_members).sortby('member')\n",
    "da_region = da_region.sel(member=common_members).sortby('member')\n",
    "ensemble_dict = sort_by_IV_ensemble(common_members)\n",
    "common_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ensemble_size_min != 1:\n",
    "    ensemble_members = list()\n",
    "    for _source in ensemble_dict.keys():\n",
    "        for _experiment in ensemble_dict[_source].keys():\n",
    "            for _config in ensemble_dict[_source][_experiment].keys():\n",
    "                if len(ensemble_dict[_source][_experiment][_config]) >= ensemble_size_min:\n",
    "                    ensemble_members = ensemble_members + ensemble_dict[_source][_experiment][_config]\n",
    "    ensemble_members.sort()\n",
    "    da_tas_glo = da_tas_glo.sel(member=ensemble_members).sortby('member')\n",
    "    da_region = da_region.sel(member=ensemble_members).sortby('member')\n",
    "    ensemble_dict = sort_by_IV_ensemble(ensemble_members)\n",
    "    print(len(ensemble_dict), 'models kept.')\n",
    "ensemble_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _model in ensemble_dict:\n",
    "    print(_model)\n",
    "    for _experiment in ensemble_dict[_model]:\n",
    "        for _ripf in ensemble_dict[_model][_experiment]:\n",
    "            print('', _experiment, len(ensemble_dict[_model][_experiment][_ripf]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_ensemble_dict = dict()\n",
    "for _model in ensemble_dict:\n",
    "    for _experiment in ensemble_dict[_model]:\n",
    "        for _config in ensemble_dict[_model][_experiment]:\n",
    "            if _experiment not in ssp_ensemble_dict:\n",
    "                ssp_ensemble_dict[_experiment] = ensemble_dict[_model][_experiment][_config]\n",
    "            else:\n",
    "                ssp_ensemble_dict[_experiment] = ssp_ensemble_dict[_experiment] + ensemble_dict[_model][_experiment][_config]\n",
    "ssp_ensemble_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_members = list()\n",
    "for _model in ensemble_dict:\n",
    "    for _ssp in ensemble_dict[_model]:\n",
    "        for _ripf in ensemble_dict[_model][_ssp]:\n",
    "            for _member in ensemble_dict[_model][_ssp][_ripf]:\n",
    "                _tmp = _member.split('_')[-1]\n",
    "                if _model+'_'+_tmp not in hist_members:\n",
    "                    hist_members.append(_model+'_'+_tmp)\n",
    "len(hist_members), hist_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title = '\\n(' #f_p =\n",
    "if len(ensemble_dict) != 1:\n",
    "    data_title += str(len(ensemble_dict))+' models, '\n",
    "    mean_lgd = 'Multi-model mean'\n",
    "else:\n",
    "    data_title += model[0]\n",
    "    mean_lgd = 'Ensemble mean'\n",
    "if not only_one_member:\n",
    "    data_title += str(len(hist_members))+' hist. members, '\n",
    "else: ###\n",
    "    data_title += str(len(ensemble_dict))+' hist. members, '\n",
    "if len(experiment) == 1:\n",
    "    data_title += experiment_dict[experiment[0]]['name']+', '\n",
    "elif set(experiment) == set(['hist-ssp126', 'hist-ssp245', 'hist-ssp370', 'hist-ssp585']):\n",
    "    data_title += 'all SSP, '\n",
    "if data_title[-2:] == ', ':\n",
    "    data_title = data_title[:-2]\n",
    "data_title += ')'\n",
    "if data_title == ' ()':\n",
    "    data_title  = ''\n",
    "data_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = xr.Dataset({\n",
    "    'GSAT': get_season(da_tas_glo, 'yr'),\n",
    "    variable: xr.concat(\n",
    "        [get_season(da_region, _tempo).resample(time='YS').mean(dim = 'time', keep_attrs = True).assign_coords({'temporality':_tempo}) for _tempo in temporality],\n",
    "        dim='temporality'\n",
    "    )\n",
    "})\n",
    "ds_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the anomaly over time and by level of warming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the anomaly with respect to the pre-industrial period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_ano_pi = normalisation(ds_raw, scaler=None, period=(piStr, piEnd))\n",
    "ds_ano_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the anomaly with respect to the reference period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ano = normalisation(ds_raw, scaler=None, period=(refStr, refEnd))\n",
    "ds_ano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of weighting (three-dimensional: model, scenario, model configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da_weight = get_weight(ds_ano_pi)\n",
    "da_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interannual statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {\n",
    "    'Berkeley Earth':   {'var': 'tas',      'path': dataDir+'Observations/tas_BEST_1m_global_185001-202401.nc'},\n",
    "    'GISTEMPv4':        {'var': 'tas',      'path': dataDir+'Observations/tas_index_global_GISS_188001-202412.nc'},\n",
    "    'HadCRUT5':         {'var': 'tas_mean', 'path': dataDir+'Observations/gsat_1y_HadCRUT5_185001-202407.nc'},\n",
    "    'NOAAGlobalTempv6': {'var': 'tas',      'path': dataDir+'Observations/tas_NOAAGT_1m_global_185001-202404.nc'},\n",
    "}\n",
    "\n",
    "ds_stats_obs = get_stats(obs_dict, stats_str = stats_str, stats_end = stats_end, window_size = window_size, center=True)\n",
    "ds_stats_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reanalyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rea_dict = {\n",
    "    'ERA5':         {'var': 'tas', 'path': dataDir+'Observations/tas_ERA5_1m_global_194001-202312.nc'},\n",
    "    'JRA-30C':      {'var': 'tas', 'path': dataDir+'Observations/tas_JRA30C_1m_global_194709-202501.nc'},\n",
    "    'NCEP-NCAR':    {'var': 'tas', 'path': dataDir+'Observations/tas_NCEP_1m_global_194801-202405.nc'},\n",
    "    'NOAA 20C':     {'var': 'tas', 'path': dataDir+'Observations/tas_NOAA20C_1m_global_183601-201512.nc'},\n",
    "}\n",
    "\n",
    "ds_stats_rea = get_stats(rea_dict, stats_str = stats_str, stats_end = stats_end, window_size = window_size, center=True)\n",
    "ds_stats_rea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = da_region.sel(time=slice(stats_str, stats_end)).resample(time='YS').mean(dim = 'time', keep_attrs = True)\n",
    "_data = _data - _data.rolling(time=window_size, center=True).mean()\n",
    "proj_std = _data.std(dim='time')\n",
    "proj_qdw = _data.quantile(.05, dim='time')\n",
    "proj_qup = _data.quantile(.95, dim='time')\n",
    "\n",
    "_members_hist = list()\n",
    "\n",
    "std_list = list()\n",
    "qdw_list = list()\n",
    "qup_list = list()\n",
    "\n",
    "for i, _source in enumerate(ensemble_dict):\n",
    "    _members = list()\n",
    "    for _experiment in ensemble_dict[_source]:\n",
    "        for _config in ensemble_dict[_source][_experiment]:\n",
    "            _tmp = ensemble_dict[_source][_experiment][_config]\n",
    "            if len(_tmp) > len(_members):\n",
    "                _members = _tmp\n",
    "    _members_hist += _members\n",
    "\n",
    "    std_list.append(xr.DataArray(\n",
    "        [[proj_std.sel(member=_members).mean().values,\n",
    "          proj_std.sel(member=_members).quantile(.05).values,\n",
    "          proj_std.sel(member=_members).quantile(.95).values]],\n",
    "        dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', [_source]), 'size': ('s', [len(_members)])}\n",
    "        ))\n",
    "    qdw_list.append(xr.DataArray(\n",
    "        [[proj_qdw.sel(member=_members).mean().values,\n",
    "          proj_qdw.sel(member=_members).quantile(.05).values,\n",
    "          proj_qdw.sel(member=_members).quantile(.95).values]],\n",
    "        dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', [_source]), 'size': ('s', [len(_members)])}\n",
    "        ))\n",
    "    qup_list.append(xr.DataArray(\n",
    "        [[proj_qup.sel(member=_members).mean().values,\n",
    "          proj_qup.sel(member=_members).quantile(.05).values,\n",
    "          proj_qup.sel(member=_members).quantile(.95).values]],\n",
    "        dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', [_source]), 'size': ('s', [len(_members)])}\n",
    "        ))\n",
    "\n",
    "_weights = da_weight.sel(member=_members_hist)\n",
    "std_list.append(xr.DataArray(\n",
    "    [[proj_std.sel(member=_members_hist).weighted(_weights).mean().values,\n",
    "      proj_std.sel(member=_members_hist).weighted(_weights).quantile(.05).values,\n",
    "      proj_std.sel(member=_members_hist).weighted(_weights).quantile(.95).values]],\n",
    "    dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', ['CMIP6']), 'size': ('s', [len(_weights)])}\n",
    "    ))\n",
    "qdw_list.append(xr.DataArray(\n",
    "    [[proj_qdw.sel(member=_members_hist).weighted(_weights).mean().values,\n",
    "      proj_qdw.sel(member=_members_hist).weighted(_weights).quantile(.05).values,\n",
    "      proj_qdw.sel(member=_members_hist).weighted(_weights).quantile(.95).values]],\n",
    "    dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', ['CMIP6']), 'size': ('s', [len(_weights)])}\n",
    "    ))\n",
    "qup_list.append(xr.DataArray(\n",
    "    [[proj_qup.sel(member=_members_hist).weighted(_weights).mean().values,\n",
    "      proj_qup.sel(member=_members_hist).weighted(_weights).quantile(.05).values,\n",
    "      proj_qup.sel(member=_members_hist).weighted(_weights).quantile(.95).values]],\n",
    "    dims=['s', 'stats'], coords={'stats': ['Average', '5%', '95%'], 'source': ('s', ['CMIP6']), 'size': ('s', [len(_weights)])}\n",
    "    ))\n",
    "\n",
    "ds_gsat_model_interannualvariability_stats = xr.Dataset(\n",
    "    {'std': xr.concat(std_list, dim='s'),\n",
    "    '5%': xr.concat(qdw_list, dim='s'),\n",
    "    '95%': xr.concat(qup_list, dim='s'),\n",
    "    }\n",
    ")\n",
    "ds_gsat_model_interannualvariability_stats = ds_gsat_model_interannualvariability_stats.set_index(s=['source', 'size'])\n",
    "ds_gsat_model_interannualvariability_stats = ds_gsat_model_interannualvariability_stats.sortby(ds_gsat_model_interannualvariability_stats['std'].sel(stats='Average'))\n",
    "ds_gsat_model_interannualvariability_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return period statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrained by observed interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_min = np.inf ; cont_max = - np.inf\n",
    "if 'obs' in constraint:\n",
    "    cont_min = min(cont_min, ds_stats_obs['std'].min().values)\n",
    "    cont_max = max(cont_max, ds_stats_obs['std'].max().values)\n",
    "if 'rea' in constraint:\n",
    "    cont_min = min(cont_min, ds_stats_rea['std'].min().values)\n",
    "    cont_max = max(cont_max, ds_stats_rea['std'].max().values)\n",
    "cont_min, cont_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint to keep only members with at least one member with an inter-annual variance within the observations range\n",
    "constrain_models = list()\n",
    "tested_std = proj_std.set_index(m=['member', 'source_id'])\n",
    "for _source in ds_gsat_model_interannualvariability_stats['source'].values:\n",
    "    if _source not in ['CMIP6']:\n",
    "        _tmp = tested_std.sel(source_id=_source)\n",
    "        if not (_tmp.min() > cont_max or _tmp.max() < cont_min):\n",
    "            constrain_models.append(_source)\n",
    "len(constrain_models), constrain_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint to keep only members with an inter-annual variance within the observations range\n",
    "constrain_members = xr.where(proj_std > cont_min, 1., np.nan).where(proj_std < cont_max, 1., np.nan).dropna(dim='member', how='all').member\n",
    "constrain_members.attrs['n_models'] = len(set(constrain_members.source_id.values))\n",
    "constrain_members.attrs['n_members'] = len(set(constrain_members.member.values))\n",
    "constrain_members = constrain_members.drop_vars(['source_id', 'experiment_id', 'member_id'])\n",
    "constrain_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask = xr.Dataset({\n",
    "    'GWL': map_global_warming_levels(ds_ano_pi.GSAT, window=window_size, levels=gwl_lvls, method=gwl_method, interval=gwl_interval)\n",
    "})\n",
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachedMem = set(xr.where(ds_mask['GWL'], 1., np.nan).dropna('member', how='all').member.values)\n",
    "notReachedMem = list(set(ds_mask.member.values).difference(reachedMem)) ; notReachedMem.sort()\n",
    "print('Members that did not reach the targetted GWL ('+str(len(notReachedMem))+'):')\n",
    "for _member in notReachedMem:\n",
    "    print(_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mask = ds_mask.assign({\n",
    "    'cnstMod': xr.where(ds_ano_pi.member.where(ds_ano_pi.source_id.isin(constrain_models)).notnull(), True, False),\n",
    "    'cnstMem': xr.where(ds_ano_pi.member.where(ds_ano_pi.member.isin(constrain_members)).notnull(), True, False),\n",
    "})\n",
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setMod = set(xr.where(ds_mask['cnstMod'], 1., np.nan).where(ds_mask['GWL']).dropna('member', how='all').source_id.values)\n",
    "setMem = set(xr.where(ds_mask['cnstMem'], 1., np.nan).where(ds_mask['GWL']).dropna('member', how='all').source_id.values)\n",
    "notReachedMod = list(setMod.difference(setMem)) ; notReachedMod.sort()\n",
    "print('Models where all members that have a correct inter-annual variability do not reach the targetted GWL:')\n",
    "for _model in notReachedMod:\n",
    "    print(_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrained by internal variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_list = list()\n",
    "for _driver in driver:\n",
    "    list_of_simulations = glob.glob(dataDir+'Simulations/'+driver_dict[_driver]['variable']+'_mon_*_hist-ssp*_index_*.nc')\n",
    "    list_of_simulations.sort()\n",
    "    print(len(list_of_simulations), list_of_simulations)\n",
    "    _tmp_driver = list()\n",
    "    for _file in list_of_simulations:\n",
    "        _member = '_'.join(_file.split('/')[-1].split('_')[2:5])\n",
    "        if _member in da_region.member:\n",
    "            _da = xr.open_dataarray(_file).assign_coords({'member': _member})\n",
    "            for _drop in ['height', 'month']:\n",
    "                try:\n",
    "                    _da = _da.drop_vars(_drop)\n",
    "                except:\n",
    "                    pass\n",
    "            _da['time'] = _da['time'].astype('datetime64[ns]')\n",
    "            _tmp_driver.append(_da)\n",
    "    _da_driver = xr.concat(_tmp_driver, dim='member').dropna(dim='member', how='all').resample(time='MS').mean(dim = 'time', keep_attrs = True)\n",
    "    _da_driver.name = _driver\n",
    "    _da_driver = get_season(_da_driver, driver_dict[_driver]['tempo'])\n",
    "    if driver_dict[_driver]['lag'] > 0:\n",
    "        _da_driver = _da_driver.isel(time=slice(0, -driver_dict[_driver]['lag'])).assign_coords({'time': _da_driver.time.isel(time=slice(driver_dict[_driver]['lag'], None))}).resample(time='YS').mean(dim = 'time', keep_attrs = True)\n",
    "    if 'amv' in _driver:\n",
    "        _da_driver = xr.DataArray(\n",
    "            data = np.apply_along_axis(\n",
    "                func1d = statsmodels.api.nonparametric.lowess, axis =_da_driver.get_axis_num('time'), arr = _da_driver,\n",
    "                exog = _da_driver.time.dt.year, return_sorted = False, frac = 10/len(_da_driver.time)),\n",
    "            coords = _da_driver.coords, dims = _da_driver.dims, name = _da_driver.name, attrs = _da_driver.attrs)\n",
    "    drivers_list.append(remove_forced_response(_da_driver, min_size = 5))\n",
    "ds_driver = xr.merge(drivers_list).assign_coords({'source_id': da_region.source_id})\n",
    "ds_driver # should take less than 1'30''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_std = ds_driver.std(dim='time')\n",
    "driver_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _driver in ds_driver:\n",
    "    ds_mask = ds_mask.assign({_driver+'+': xr.where(ds_driver[_driver] > driver_std[_driver], True, False)})\n",
    "    ds_mask = ds_mask.assign({_driver+'~': xr.where((ds_driver[_driver] < driver_std[_driver]) & (ds_driver[_driver] > - driver_std[_driver]), True, False)})\n",
    "    ds_mask = ds_mask.assign({_driver+'-': xr.where(ds_driver[_driver] < - driver_std[_driver], True, False)})\n",
    "ds_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_obs_list = list()\n",
    "for _driver in ['amvGlob', 'nino34']:\n",
    "    _file = dataDir+'Observations/'+_driver+'_mon_ERSSTv5_index_*.nc'\n",
    "    if 'amv' in _driver:\n",
    "        _file = _file.replace(_driver+'_', 'amv_').replace('_mon_', '_yr_').replace('.nc', '_unfiltered.nc')\n",
    "    print(_file)\n",
    "    _da = xr.open_mfdataset(_file).load().get(_driver)\n",
    "    for _drop in ['height', 'month']:\n",
    "        try:\n",
    "            _da = _da.drop_vars(_drop)\n",
    "        except:\n",
    "            pass\n",
    "    _da['time'] = _da['time'].astype('datetime64[ns]')\n",
    "    _da_driver_obs = _da ; _da_driver_obs.name = _driver\n",
    "    if 'amv' not in _driver:\n",
    "        _da_driver_obs = _da_driver_obs.resample(time='MS').mean(dim = 'time', keep_attrs = True)\n",
    "        _da_driver_obs = get_season(_da_driver_obs, driver_dict[_driver]['tempo'])\n",
    "    if driver_dict[_driver]['lag'] > 0:\n",
    "       _da_driver_obs = _da_driver_obs.isel(time=slice(0, -driver_dict[_driver]['lag'])).assign_coords({'time': _da_driver_obs.time.isel(time=slice(driver_dict[_driver]['lag'], None))}).resample(time='YS').mean(dim = 'time', keep_attrs = True)\n",
    "    if 'amv' in _driver:\n",
    "        _da_driver_obs = xr.DataArray(\n",
    "            data = np.apply_along_axis(\n",
    "                func1d = statsmodels.api.nonparametric.lowess, axis =_da_driver_obs.get_axis_num('time'), arr = _da_driver_obs,\n",
    "                exog = _da_driver_obs.time.dt.year, return_sorted = False, frac = 10/len(_da_driver_obs.time)),\n",
    "            coords = _da_driver_obs.coords, dims = _da_driver_obs.dims, name = _da_driver_obs.name, attrs = _da_driver_obs.attrs)\n",
    "    elif 'nino' in _driver:\n",
    "        _da_driver_obs = _da_driver_obs - get_trend(_da_driver_obs)\n",
    "    drivers_obs_list.append(_da_driver_obs.sel(time=slice('1940', '2024')).resample(time='YS').mean(dim = 'time', keep_attrs = True))\n",
    "ds_driver_obs = xr.merge(drivers_obs_list)\n",
    "ds_driver_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_GWL = xr.Dataset({variable: ds_ano_pi[variable].where(ds_mask['GWL'])})\n",
    "\n",
    "for i, _driver in enumerate(ds_driver):\n",
    "    for _tier in ['+', '~', '-']:\n",
    "        ds_GWL = ds_GWL.assign({variable+'_'+_driver+_tier: ds_GWL[variable].where(ds_mask[_driver+_tier])})\n",
    "        for _driver2 in ds_driver:\n",
    "            if _driver2 != _driver and _driver2 in driver[i:]:\n",
    "                for _tier2 in ['+', '~', '-']:\n",
    "                    ds_GWL = ds_GWL.assign({\n",
    "                        variable+'_'+_driver+_tier+'_'+_driver2+_tier2: ds_GWL[variable].where(ds_mask[_driver+_tier]).where(ds_mask[_driver2+_tier2])\n",
    "                        })\n",
    "\n",
    "for _var in ds_GWL:\n",
    "    ds_GWL = ds_GWL.assign({_var+'_kMod': ds_GWL[_var].where(ds_mask['cnstMod'])})\n",
    "    ds_GWL = ds_GWL.assign({_var+'_kMem': ds_GWL[_var].where(ds_mask['cnstMem'])})\n",
    "\n",
    "for _var in ds_GWL:\n",
    "    _tmp = ds_GWL[_var].dropna(dim='member', how='all')\n",
    "    ds_GWL[_var].attrs['n_models'] = len(set(_tmp.source_id.values))\n",
    "    ds_GWL[_var].attrs['n_members'] = len(set(_tmp.member.values))\n",
    "\n",
    "ds_GWL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return period computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rtn_list = list()\n",
    "\n",
    "for _obs in observations:\n",
    "    _tmp = list()\n",
    "    _tempo = list(observations[_obs].keys())[0]\n",
    "    _event = observations[_obs][_tempo]\n",
    "\n",
    "    ### PI\n",
    "    _data = ds_ano[variable].sel(time=slice(piStr, piEnd), temporality=_tempo)\n",
    "\n",
    "    _da = _data.stack(sample=['member', 'time']).dropna('sample', how='all')\n",
    "    _da_weight = da_weight.expand_dims(\n",
    "        {'time': ds_ano.time.sel(time=slice(piStr, piEnd))}\n",
    "    ).transpose(..., 'time').stack(sample=['member', 'time']).sel(sample=_da.sample)\n",
    "    _da_weight, _da = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "    _data_boot = compute_bootstrap(_data, da_weight.sel(member=_data.member), n_boot=n_boot).stack(sample=['member', 'time'])\n",
    "\n",
    "    _tmp.append(xr.DataArray(\n",
    "        return_period(data = _da, weight = _da_weight, event = observations[_obs][_tempo], bootstrap = bootstrap, data_boot = _data_boot, low=int_low, high=int_high),\n",
    "        dims = ['return_period'],\n",
    "        coords = {\n",
    "            'return_period': ['computed', 'normal', 'skew-normal' , 'bootstrap-low', 'bootstrap-med', 'bootstrap-high'],\n",
    "            'dataset': variable+'-past',\n",
    "            'size': len(_da.member),\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    ### Currend GWL\n",
    "    for _data_name in [\n",
    "        variable,\n",
    "        variable+'_kMod',\n",
    "        variable+'_amv+_nino34+_kMod',\n",
    "        variable+'_amv+_nino34-_kMod',\n",
    "        variable+'_amv~_nino34~_kMod',\n",
    "        variable+'_kMem',\n",
    "    ]:\n",
    "        _data = ds_GWL.get(_data_name).sel(temporality=_tempo).isel(warming_level=0)\n",
    "        _da_weight = local_weights(xr.where(_data, True, False), method = 'occurrence-per-model').expand_dims(dim={'time': _data.time})\n",
    "\n",
    "        _da = _data.stack(sample=('member', 'time')).dropna('sample')\n",
    "        _da_weight = _da_weight.stack(sample=('member', 'time')).sel(sample=_da.sample)\n",
    "        \n",
    "        _da_weight, _da  = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "\n",
    "        _data_boot = compute_bootstrap(_da, _da_weight, n_boot=n_boot, dim='sample') if bootstrap else None\n",
    "\n",
    "        _tmp.append(xr.DataArray(\n",
    "            return_period(data = _da, weight = _da_weight, event = observations[_obs][_tempo], bootstrap = bootstrap, data_boot = _data_boot, low=int_low, high=int_high),\n",
    "            dims = ['return_period'],\n",
    "            coords = {\n",
    "                'return_period': ['computed', 'normal', 'skew-normal' , 'bootstrap-low', 'bootstrap-med', 'bootstrap-high'],\n",
    "                'dataset': _data_name,\n",
    "                'size': len(_da.member),\n",
    "            }\n",
    "        ))\n",
    "    _rtn_list.append(xr.concat(_tmp, dim = 'dataset'))\n",
    "    _rtn_list[-1].name = _obs\n",
    "\n",
    "ds_rtn = xr.merge(_rtn_list)\n",
    "ds_rtn # should take a bit more than 10' (sets=21 x n_boot=1000), less than 15'' without bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-annual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_c = 'b'\n",
    "rea_c = 'r'\n",
    "for _cnstrn in (None, 'models', 'members'):\n",
    "    _suffix = ''\n",
    "    if _cnstrn == 'models':\n",
    "        _suffix += '_kMod'\n",
    "    elif _cnstrn == 'members':\n",
    "        _suffix += '_kMem'\n",
    "\n",
    "    fig, ax = plt.subplots(2,1, figsize=(10, 10))\n",
    "    fig.suptitle('Statistics of annual residuals (10-year window)\\nof global surface air temperature ('+stats_str+'-'+stats_end+')', fontsize=15)\n",
    "    ax[1].axhline(y=0, ls='-', lw=.75, c='k')\n",
    "\n",
    "    xticks = [0] ; btm_lbls = list() ; upr_lbls = list()\n",
    "\n",
    "    if 'obs' in constraint:\n",
    "        for _source in ds_stats_obs.source:\n",
    "            btm_lbls.append(_source.values) ; upr_lbls.append('Obs.')\n",
    "            plot_stats(xticks[-1], ds_stats_obs['std'].sel(source=_source), ds_stats_obs['qdw'].sel(source=_source), ds_stats_obs['qup'].sel(source=_source), ax, obs_c)\n",
    "            xticks.append(xticks[-1]+1)\n",
    "        ax[0].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "        ax[1].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "        xticks[-1] += 1\n",
    "\n",
    "    if 'rea' in constraint:\n",
    "        for _source in ds_stats_rea.source:\n",
    "            btm_lbls.append(_source.values) ; upr_lbls.append('Rea.')\n",
    "            plot_stats(xticks[-1], ds_stats_rea['std'].sel(source=_source), ds_stats_rea['qdw'].sel(source=_source), ds_stats_rea['qup'].sel(source=_source), ax, rea_c)\n",
    "            xticks.append(xticks[-1]+1)\n",
    "        ax[0].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "        ax[1].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "        xticks[-1] += 1\n",
    "\n",
    "    _source = 'CMIP6'\n",
    "    _da = ds_gsat_model_interannualvariability_stats.sel(source='CMIP6').isel(size=0)\n",
    "    btm_lbls.append('CMIP6') ; upr_lbls.append(int(_da['size']))\n",
    "    _xlab_CMIP = len(upr_lbls) - 1\n",
    "    _members = proj_std.member\n",
    "    if _cnstrn == 'models':\n",
    "        _loc_std = proj_std.where(ds_mask['cnstMod'].sel(member=_members))\n",
    "        _loc_qdw = proj_qdw.where(ds_mask['cnstMod'].sel(member=_members))\n",
    "        _loc_qup = proj_qup.where(ds_mask['cnstMod'].sel(member=_members))\n",
    "    if _cnstrn == 'members':\n",
    "        _loc_std = proj_std.where(ds_mask['cnstMem'].sel(member=_members))\n",
    "        _loc_qdw = proj_qdw.where(ds_mask['cnstMem'].sel(member=_members))\n",
    "        _loc_qup = proj_qup.where(ds_mask['cnstMem'].sel(member=_members))\n",
    "    else:\n",
    "        _loc_std = proj_std\n",
    "        _loc_qdw = proj_qdw\n",
    "        _loc_qup = proj_qup\n",
    "    _local_weights = local_weights(_loc_std)\n",
    "    boxplot([_loc_std.values], ax=ax[0], dx=xticks[-1], weights=[_local_weights.values], bar='mean', outliers=False, color='k')\n",
    "    boxplot([_loc_qdw.values], ax=ax[1], dx=xticks[-1], weights=[_local_weights.values], bar='mean', outliers=False, color='k')\n",
    "    boxplot([_loc_qup.values], ax=ax[1], dx=xticks[-1], weights=[_local_weights.values], bar='mean', outliers=False, color='k')\n",
    "    xticks.append(xticks[-1]+1)\n",
    "\n",
    "    ax[0].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "    ax[1].axvline(x=xticks[-1], lw=.75, c='k')\n",
    "    xticks[-1] += 1\n",
    "\n",
    "    _nMod = upr_lbls[_xlab_CMIP] ; _nMem = 0\n",
    "    for _source in ds_gsat_model_interannualvariability_stats['source'].values:\n",
    "        if _source not in ['CMIP6']:\n",
    "            _members = list()\n",
    "            for _experiment in ensemble_dict[_source]:\n",
    "                for _config in ensemble_dict[_source][_experiment]:\n",
    "                    _tmp = ensemble_dict[_source][_experiment][_config]\n",
    "                    if len(_tmp) > len(_members):\n",
    "                        _members = _tmp\n",
    "            n_proj = len(_members)\n",
    "\n",
    "            _mks = [['o', 'v', '^'] for _ in range(n_proj)]\n",
    "            _prefix=''\n",
    "            if _source not in constrain_models and _cnstrn == 'models':\n",
    "                _nMod -= n_proj\n",
    "                _mks = [['x', 'x', 'x'] for _ in range(n_proj)]\n",
    "            elif _cnstrn == 'members':\n",
    "                _mks = list() ; _nKeep = 0\n",
    "                for _member in _members:\n",
    "                    if _member in constrain_members:\n",
    "                        _nKeep += 1\n",
    "                        _mks.append(['o', 'v', '^'])\n",
    "                    else:\n",
    "                        _mks.append(['x', 'x', 'x'])\n",
    "                _prefix=str(_nKeep)+'/'\n",
    "                _nMem += _nKeep\n",
    "            _mks = np.array(_mks)\n",
    "            btm_lbls.append(_source) ; upr_lbls.append(_prefix+str(n_proj))\n",
    "            \n",
    "            for i, _member in enumerate(_members):\n",
    "                ax[0].plot(xticks[-1], proj_std.sel(member=_member), _mks[i,0][0], c='k', ms=3, zorder=2)\n",
    "                ax[1].plot(xticks[-1], proj_qdw.sel(member=_member), _mks[i,1][0], c='k', ms=3, zorder=2)\n",
    "                ax[1].plot(xticks[-1], proj_qup.sel(member=_member), _mks[i,2][0], c='k', ms=3, zorder=2)\n",
    "\n",
    "            xticks.append(xticks[-1]+1)\n",
    "    \n",
    "    if _cnstrn == 'models':\n",
    "        upr_lbls[_xlab_CMIP] = str(_nMod)+'/'+str(upr_lbls[_xlab_CMIP])\n",
    "    elif _cnstrn == 'members':\n",
    "        upr_lbls[_xlab_CMIP] = str(_nMem)+'/'+str(upr_lbls[_xlab_CMIP])\n",
    "    \n",
    "    xticks = xticks[:-1]\n",
    "\n",
    "    ax[0].set_ylabel('Standard deviation', fontsize=13)\n",
    "    ax[0].xaxis.tick_top()\n",
    "    ax[0].set_xticks(xticks, upr_lbls, rotation=90)\n",
    "    ax[0].set_xlim(-1, xticks[-1]+1)\n",
    "    ax[1].set_ylabel('5-95% Range', fontsize=13)\n",
    "    ax[1].set_xlabel('Dataset', fontsize=13)\n",
    "    ax[1].set_xticks(xticks, btm_lbls, rotation=90)\n",
    "    ax[1].set_xlim(-1, xticks[-1]+1)\n",
    "    outFig = 'interanualVariability-obsReaCMIPmodels'\n",
    "    outFig += _suffix\n",
    "    plt.savefig(outDir+saveName.replace('_tempo_', '_yr_')+'/'+outFig+extension)\n",
    "    print('Saved:', outDir+outFig+extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single year detection and attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw and constrained by observed inter-annual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count and show number of models!\n",
    "\n",
    "colors = ['#364B9A', '#F67E4B', '#A50026']\n",
    "linestyles = ['-', '-.', '--', '-']\n",
    "lw = 3\n",
    "\n",
    "for _past, _current in [\n",
    "        [ds_ano[variable], ds_GWL[variable]],\n",
    "        [ds_ano[variable].where(ds_ano.source_id.isin(constrain_models)), ds_GWL[variable+'_kMod']],\n",
    "        [ds_ano[variable].where(ds_ano.member.isin(constrain_members)), ds_GWL[variable+'_kMem']],\n",
    "    ]:\n",
    "    reg_mod = dict()\n",
    "    for _tempo in temporality:\n",
    "\n",
    "        _range = (-0.75, 2.00); _nbins = int((_range[1]-_range[0]) * 20)\n",
    "\n",
    "        rtn_box = [Line2D([0], [0], label='Return period:', ls='')]\n",
    "        text_list = list()\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "        _first_line = 'Past and current '+variable_dict[variable]+' probability ('+str(window_size-1)+'-year window)'\n",
    "        if '_kMod' in _current.name or '_kMem' in _current.name: ###\n",
    "            _second_line = 'from constrained CMIP6 ('+str(len(set(_current.dropna(dim='member', how='all').source_id.values)))+' models, '+str(len(_current.dropna(dim='member', how='all').member))+' members)'\n",
    "        else:\n",
    "            _second_line = 'from CMIP6 ('+str(len(set(_current.dropna(dim='member', how='all').source_id.values)))+' models, '+str(len(_current.dropna(dim='member', how='all').member))+' members)'\n",
    "        plt.title(_first_line+'\\n'+_second_line, fontsize=15)\n",
    "        ax.set_xlabel(temporality_dict[_tempo]+' '+variable_dict[variable]+' anomaly (°C), reference period: '+piStr+'-'+piEnd, fontsize=15)\n",
    "        ax.set_ylabel('Occurrence (%)', fontsize=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "        ax.axvline(x=0, ls='-', c='k', lw=.5)\n",
    "\n",
    "        ax.set_xlim(_range)\n",
    "        if window_size == 21:\n",
    "            ax.set_ylim((0.0, 2.0))\n",
    "        elif window_size == 11:\n",
    "            ax.set_ylim((0.0, 4.0))\n",
    "        else:\n",
    "            ax.set_ylim((0.0, 2.0))\n",
    "        plt.locator_params(axis='y', nbins=10)\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(xmax=_nbins/(_range[1]-_range[0]), decimals=1, symbol=0))\n",
    "        _xlims= ax.get_xlim(); _ylims= ax.get_ylim()\n",
    "        _saveName = _tempo.join(saveName.split('tempo'))+'-reg_anom_distributions_ref-gwl-obs'\n",
    "\n",
    "        ###\n",
    "        i = 0\n",
    "        _da = _past.sel(time=slice(piStr, piEnd), temporality=_tempo)\n",
    "        _da_weight = local_weights(_da)\n",
    "        _da = _da.stack(sample=['member', 'time'])\n",
    "        _da_weight = _da_weight.expand_dims(\n",
    "            {'time': ds_ano.time.sel(time=slice(piStr, piEnd))}\n",
    "        ).transpose(..., 'time').stack(sample=['member', 'time']).sel(sample=_da.sample)\n",
    "        _da_weight, _da = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "        print('Data size:', len(_da.member))\n",
    "\n",
    "        plt.hist(_da, weights=_da_weight,\n",
    "                    density=True, bins=_nbins, range=_range,\n",
    "                    histtype='stepfilled', alpha=0.5, color=colors[i], label=ensemble_lgd+' '+piStr+'-'+piEnd,\n",
    "                    edgecolor=colors[i])\n",
    "        ax.axvline(x=_da.weighted(_da_weight).mean(), ls='-', c=colors[i], lw=2, zorder=2)\n",
    "\n",
    "        if show_gauss:\n",
    "            _mean = _da.weighted(_da_weight).mean()\n",
    "            _std = _da.weighted(_da_weight).std()\n",
    "            _x = np.linspace(_da.min(), _da.max(), 100)\n",
    "            ax.plot(_x, stats.norm.pdf(_x, loc=_mean, scale=_std), color=colors[i])\n",
    "\n",
    "        ###\n",
    "        i = 1\n",
    "        _wrm_lvl = targets[_tempo]['GWL']\n",
    "        _legend=ensemble_lgd+' GWL = {:.2f}°C'.format(targets[_tempo]['GWL'])\n",
    "\n",
    "        _tmp = _current.sel(temporality=_tempo)\n",
    "\n",
    "        _da = _tmp.sel(warming_level=_wrm_lvl)\n",
    "        _da_weight = local_weights(_da).expand_dims(dim={'time': _current.time})\n",
    "        \n",
    "        _da = _da.stack(sample=['member', 'time']).dropna('sample')\n",
    "        _da_weight = _da_weight.stack(sample=('member', 'time')).sel(sample=_da.sample)\n",
    "        \n",
    "        _da_weight, _da  = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "        print('Data size:', len(_da.member))\n",
    "\n",
    "        plt.hist(_da, weights=_da_weight,\n",
    "                    density=True, bins=_nbins, range=_range,\n",
    "                    histtype='stepfilled', alpha=0.5, color=colors[i], label=_legend,\n",
    "                    edgecolor=colors[i])\n",
    "        ax.axvline(x=_da.weighted(_da_weight).mean(), ls='-', c=colors[i], lw=2, zorder=2)\n",
    "\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        _lgd = ax.legend(h, l, fontsize=12, loc='upper left')\n",
    "\n",
    "        if show_gauss:\n",
    "            _mean = _da.weighted(_da_weight).mean()\n",
    "            _std = _da.weighted(_da_weight).std()\n",
    "            _x = np.linspace(_da.min(), _da.max(), 100)\n",
    "            ax.plot(_x, stats.norm.pdf(_x, loc=_mean, scale=_std), color=colors[i])\n",
    "\n",
    "        for iObs, _obs in enumerate(observations):\n",
    "            if _tempo in observations[_obs].keys():\n",
    "                _text = ('{0:.0f} years'.format(ds_rtn[_obs].sel(return_period='computed', dataset=_current.name)))\n",
    "                if bootstrap:\n",
    "                    try:\n",
    "                        _text += (\n",
    "                            ' [{0:.0f} - '.format(min(ds_rtn[_obs].sel(return_period='computed', dataset=_current.name)-1, ds_rtn[_obs].sel(return_period='bootstrap-low', dataset=_current.name)))\n",
    "                            +'{0:.0f}]'.format(max(ds_rtn[_obs].sel(return_period='computed', dataset=_current.name)+1, ds_rtn[_obs].sel(return_period='bootstrap-high', dataset=_current.name)))\n",
    "                            )\n",
    "                    except OverflowError:\n",
    "                        pass\n",
    "                rtn_box.append(Line2D([0], [0], label=_text, ls=linestyles[iObs], color=colors[i], lw=lw))\n",
    "\n",
    "        for iObs, _obs in enumerate(observations):\n",
    "            if _tempo in observations[_obs].keys():\n",
    "                ax.axvline(x=observations[_obs][_tempo], ls=linestyles[iObs], c='red', lw=lw,\n",
    "                )\n",
    "                ax.text(observations[_obs][_tempo], 0, ' '+targets[_tempo]['year']+' '+lgd_obs_dict[_obs], fontsize=12, c='red', va='bottom', ha='right', rotation=90)\n",
    "                ax.add_artist(ax.legend(handles=rtn_box, loc='upper right', fontsize=12))\n",
    "\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        _lgd = ax.legend(h, l, fontsize=12, loc='upper left')\n",
    "\n",
    "        _saveFig = outDir+_tempo.join(saveName.split('tempo'))+'/'+_saveName+'_'+_current.name+extension\n",
    "        plt.savefig(_saveFig)\n",
    "        print('Saved:', _saveFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrained by drivers of internal variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2store = ds_rtn.copy(deep=True)\n",
    "\n",
    "colors = {\n",
    "    'high': '#EE99AA',\n",
    "    'neutral': 'dimgrey',\n",
    "    'low': '#6699CC',\n",
    "}\n",
    "linestyles = ['-', '-.', '--', '-']\n",
    "lw = 3\n",
    "\n",
    "fig_dict = {\n",
    "    variable:                   {'legend': ensemble_lgd+' (all members)', 'color': colors['neutral']},\n",
    "    variable+'_nino34+':        {'legend': ensemble_lgd+' El Niño', 'color': colors['high']},\n",
    "    variable+'_nino34~':        {'legend': ensemble_lgd+' Neutral NINO3.4', 'color': colors['neutral']},\n",
    "    variable+'_nino34-':        {'legend': ensemble_lgd+' La Niña', 'color': colors['low']},\n",
    "    variable+'_amv+':           {'legend': ensemble_lgd+' AMV+', 'color': colors['high']},\n",
    "    variable+'_amv~':           {'legend': ensemble_lgd+' Neutral AMV', 'color': colors['neutral']},\n",
    "    variable+'_amv-':           {'legend': ensemble_lgd+' AMV-', 'color': colors['low']},\n",
    "    variable+'_amv+_nino34+':   {'legend': ensemble_lgd+' AMV+ & El Niño', 'color': colors['high']},\n",
    "    variable+'_amv+_nino34-':   {'legend': ensemble_lgd+' AMV+ & La Niña', 'color': colors['low']},\n",
    "    variable+'_amv~_nino34~':   {'legend': ensemble_lgd+' Neutral AMV & NINO3.4', 'color': colors['neutral']},\n",
    "}\n",
    "_list_of_keys = list(fig_dict.keys())\n",
    "for _cont in ['_kMod', '_kMem']:\n",
    "    for _key in _list_of_keys:\n",
    "        fig_dict[_key+_cont] = fig_dict[_key]\n",
    "\n",
    "for _sets in [\n",
    "    [variable+'_amv+_nino34+_kMod', variable+'_kMod'],\n",
    "    [variable+'_amv+_nino34-_kMod', variable+'_kMod'],\n",
    "]:\n",
    "    reg_mod = dict()\n",
    "    for _tempo in temporality:\n",
    "        _wrm_lvl = targets[_tempo]['GWL']\n",
    "\n",
    "        _range = (.8, 2.); _nbins = int((_range[1]-_range[0]) * 20)\n",
    "\n",
    "        rtn_box = [Line2D([0], [0], label='Return period:', ls='')]\n",
    "        text_list = list()\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "        ax.set_xlim(_range)\n",
    "        if window_size == 21:\n",
    "            ax.set_ylim((0.0, 2.0))\n",
    "        elif window_size == 11:\n",
    "            ax.set_ylim((0.0, 4.0))\n",
    "        else:\n",
    "            ax.set_ylim((0.0, 2.0))\n",
    "        plt.locator_params(axis='y', nbins=10)\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(xmax=_nbins/(_range[1]-_range[0]), decimals=1, symbol=0))\n",
    "        _xlims= ax.get_xlim(); _ylims= ax.get_ylim()\n",
    "        _saveName = _tempo.join(saveName.split('tempo'))+'-reg_anom_distributions_ref-gwl-obs_totalConstraint'\n",
    "        if bootstrap:\n",
    "            _saveName += '_boot'+str(n_boot)\n",
    "\n",
    "        _kept_models = set() ; _kept_members = set()\n",
    "        _n_members = 0\n",
    "        for i, _set in enumerate(_sets):\n",
    "\n",
    "            _data = ds_GWL[_set].sel(temporality=_tempo).sel(warming_level=_wrm_lvl, method='nearest')\n",
    "            _da_weight = local_weights(xr.where(_data, True, False), method = 'occurrence-per-model').expand_dims(dim={'time': _data.time}).transpose(..., 'time')\n",
    "            if _set not in data2store:\n",
    "                print(_set)\n",
    "                data2store = data2store.assign({_set: _data.copy(deep=True), _set+'_weight': _da_weight.copy(deep=True)})\n",
    "            _da = _data.stack(sample=('member', 'time')).dropna('sample')\n",
    "            _da_weight = _da_weight.stack(sample=('member', 'time')).sel(sample=_da.sample)\n",
    "            _da_weight, _da  = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "            print('Data size:', len(_da.member))\n",
    "            _n_members += int(len(_da.member) / window_size)\n",
    "            _kept_models.update(list(_da.source_id.values))\n",
    "            _kept_members.update(list(_da.member.values))\n",
    "\n",
    "            plt.hist(_da, weights=_da_weight,\n",
    "                        density=True, bins=_nbins, range=_range,\n",
    "                        histtype='stepfilled', alpha=0.5, color=fig_dict[_set]['color'], label=fig_dict[_set]['legend'],\n",
    "                        edgecolor=fig_dict[_set]['color'])\n",
    "            ax.axvline(x=_da.weighted(_da_weight).mean(), ls='-', c=fig_dict[_set]['color'], lw=1.5, zorder=2)\n",
    "\n",
    "            h, l = ax.get_legend_handles_labels()\n",
    "            _lgd = ax.legend(h, l, fontsize=12, loc='upper left')\n",
    "\n",
    "            if show_gauss:\n",
    "                _mean = _da.weighted(_da_weight).mean()\n",
    "                _std = _da.weighted(_da_weight).std()\n",
    "                _x = np.linspace(_da.min(), _da.max(), 100)\n",
    "                ax.plot(_x, stats.norm.pdf(_x, loc=_mean, scale=_std), color=fig_dict[_set]['color'])\n",
    "\n",
    "            for iObs, _obs in enumerate(observations):\n",
    "                if _tempo in observations[_obs].keys():\n",
    "                    _text = ('{0:.0f} years'.format(ds_rtn[_obs].sel(return_period='computed', dataset=_set)))\n",
    "                    if bootstrap:\n",
    "                        try:\n",
    "                            _text += (\n",
    "                                ' [{0:.0f} - '.format(min(ds_rtn[_obs].sel(return_period='computed', dataset=_set)-1, ds_rtn[_obs].sel(return_period='bootstrap-low', dataset=_set)))\n",
    "                                +'{0:.0f}]'.format(max(ds_rtn[_obs].sel(return_period='computed', dataset=_set)+1, ds_rtn[_obs].sel(return_period='bootstrap-high', dataset=_set)))\n",
    "                                )\n",
    "                        except OverflowError:\n",
    "                            pass\n",
    "                    rtn_box.append(Line2D([0], [0], label=_text, ls=linestyles[iObs], color=fig_dict[_set]['color'], lw=lw))\n",
    "\n",
    "        for iObs, _obs in enumerate(observations):\n",
    "            if _tempo in observations[_obs].keys():\n",
    "                ax.axvline(x=observations[_obs][_tempo], ls=linestyles[iObs], c='red', lw=lw,\n",
    "                )\n",
    "                ax.text(observations[_obs][_tempo], 0, ' '+targets[_tempo]['year']+' '+lgd_obs_dict[_obs]+' ', fontsize=12, c='red', va='bottom', ha='right', rotation=90)\n",
    "                ax.add_artist(ax.legend(handles=rtn_box, loc='upper right', fontsize=12))\n",
    "\n",
    "        plt.title(variable_dict[variable]+' anomaly distribution at {:.2f}°C'.format(targets[_tempo]['GWL'])+' ('+str(window_size-1)+'-year window)\\nfrom constrained CMIP6 ('+str(len(_kept_models))+' models, '+str(len(_kept_members))+' members)', fontsize=15)\n",
    "        ax.set_xlabel(temporality_dict[_tempo]+' '+variable_dict[variable]+' anomaly (°C), reference period: '+piStr+'-'+piEnd, fontsize=15)\n",
    "        ax.set_ylabel('Occurrence (%)', fontsize=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "        ax.axvline(x=0, ls='-', c='k', lw=.5)\n",
    "\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        _lgd = ax.legend(h, l, fontsize=12, loc='upper left')\n",
    "\n",
    "        _saveFig = outDir+_tempo.join(saveName.split('tempo'))+'/'+_saveName+extension\n",
    "        plt.savefig(_saveFig, transparent=True)\n",
    "        print('Saved:', _saveFig)\n",
    "data2store.to_netcdf(outDir+_tempo.join(saveName.split('tempo'))+'/'+'forster_yr'+targets[_tempo]['year']+'.nc')\n",
    "data2store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forster et al., 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection and attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2022 = xr.open_dataset(outDir+'yr'.join(saveName.split('tempo'))+'/'+'forster_yr2022.nc')\n",
    "ds_2023 = xr.open_dataset(outDir+'yr'.join(saveName.split('tempo'))+'/'+'forster_yr2023.nc')\n",
    "ds_2024 = xr.open_dataset(outDir+'yr'.join(saveName.split('tempo'))+'/'+'forster_yr2024.nc')\n",
    "sets = {\n",
    "    '2022': [variable+'_kMod', variable+'_amv+_nino34-_kMod'],\n",
    "    '2023': [variable+'_kMod', variable+'_amv+_nino34-_kMod'],\n",
    "    '2024': [variable+'_kMod', variable+'_amv+_nino34+_kMod'],\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'high': '#EE99AA',\n",
    "    'neutral': 'dimgrey',\n",
    "    'low': '#6699CC',\n",
    "}\n",
    "linestyles = ['-', '-.', '--', '-']\n",
    "lw = 3\n",
    "\n",
    "fig_dict = {\n",
    "    variable:                   {'legend': ensemble_lgd+' (all members)', 'color': colors['neutral']},\n",
    "    variable+'_nino34+':        {'legend': ensemble_lgd+' El Niño', 'color': colors['high']},\n",
    "    variable+'_nino34~':        {'legend': ensemble_lgd+' Neutral ENSO', 'color': colors['neutral']},\n",
    "    variable+'_nino34-':        {'legend': ensemble_lgd+' La Niña', 'color': colors['low']},\n",
    "    variable+'_amv+':           {'legend': ensemble_lgd+' AMV+', 'color': colors['high']},\n",
    "    variable+'_amv~':           {'legend': ensemble_lgd+' Neutral AMV', 'color': colors['neutral']},\n",
    "    variable+'_amv-':           {'legend': ensemble_lgd+' AMV-', 'color': colors['low']},\n",
    "    variable+'_amv+_nino34+':   {'legend': ensemble_lgd+' AMV+ & El Niño', 'color': colors['high']},\n",
    "    variable+'_amv+_nino34-':   {'legend': ensemble_lgd+' AMV+ & La Niña', 'color': colors['low']},\n",
    "    variable+'_amv~_nino34~':   {'legend': ensemble_lgd+' Neutral AMV & ENSO', 'color': colors['neutral']},\n",
    "}\n",
    "_list_of_keys = list(fig_dict.keys())\n",
    "for _cont in ['_kMod', '_kMem']:\n",
    "    for _key in _list_of_keys:\n",
    "        fig_dict[_key+_cont] = fig_dict[_key]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(31, 9), sharex=True, sharey=True)\n",
    "fig.suptitle(variable_dict[variable]+' interannual anomalies distribution from CMIP6 (15 models) for evolutive anthropogically-forced warming (ANT_GWL)\\nand conditional to the combined phases of OND [yr-1] ENSO and AMV modes of variability', fontsize=21, y=1.01)\n",
    "\n",
    "h, l = list(), list()\n",
    "i = 0\n",
    "for  _p,    _year, _ds,     _gwl, _wmo in [\n",
    "    ['a) ', '2024', ds_2024, 1.36, 1.52],\n",
    "    ['b) ', '2023', ds_2023, 1.31, 1.44],\n",
    "    ['c) ', '2022', ds_2022, 1.26, 1.15],\n",
    "    ]:\n",
    "\n",
    "    reg_mod = dict()\n",
    "    for _tempo in temporality:\n",
    "        _wrm_lvl = targets[_tempo]['GWL']\n",
    "\n",
    "        _range = (.8, 2.); _nbins = int((_range[1]-_range[0]) * 20)\n",
    "\n",
    "        rtn_box = [Line2D([0], [0], label='Return period:', ls='')]\n",
    "        text_list = list()\n",
    "\n",
    "        ax[i].set_xlim(_range)\n",
    "        if window_size == 21:\n",
    "            ax[i].set_ylim((0.0, 2.0))\n",
    "        elif window_size == 11:\n",
    "            ax[i].set_ylim((0.0, 4.0))\n",
    "        else:\n",
    "            ax[i].set_ylim((0.0, 2.0))\n",
    "        plt.locator_params(axis='y', nbins=10)\n",
    "        ax[i].yaxis.set_major_formatter(PercentFormatter(xmax=_nbins/(_range[1]-_range[0]), decimals=1, symbol=0))\n",
    "        _xlims= ax[i].get_xlim(); _ylims= ax[i].get_ylim()\n",
    "        _saveName = _tempo.join(saveName.split('tempo'))+'-reg_anom_distributions_ref-gwl-obs_totalConstraint'\n",
    "        if bootstrap:\n",
    "            _saveName += '_boot'+str(n_boot)\n",
    "\n",
    "        _kept_models = set() ; _kept_members = set()\n",
    "        _n_members = 0\n",
    "        for _set in sets[_year]:\n",
    "            print(_set)\n",
    "            _da, _da_weight = _ds.get(_set), _ds.get(_set+'_weight')\n",
    "            _da = _da.stack(sample=('member', 'time')).dropna('sample')\n",
    "            _da_weight = _da_weight.stack(sample=('member', 'time')).sel(sample=_da.sample)\n",
    "            _da_weight, _da  = _da_weight.isel(sample=_da.argsort().values), _da.isel(sample=_da.argsort().values)\n",
    "            print('Data size:', len(_da.member))\n",
    "\n",
    "            _n_members += int(len(_da.member) / window_size)\n",
    "            _kept_models.update(list(_da.source_id.values))\n",
    "            _kept_members.update(list(_da.member.values))\n",
    "\n",
    "            ax[i].hist(_da, weights=_da_weight,\n",
    "                        density=True, bins=_nbins, range=_range,\n",
    "                        histtype='stepfilled', alpha=0.5, color=fig_dict[_set]['color'], label=fig_dict[_set]['legend'],\n",
    "                        edgecolor=fig_dict[_set]['color'])\n",
    "\n",
    "            if show_gauss:\n",
    "                _mean = _da.weighted(_da_weight).mean()\n",
    "                _std = _da.weighted(_da_weight).std()\n",
    "                _x = np.linspace(_da.min(), _da.max(), 100)\n",
    "                ax[i].plot(_x, stats.norm.pdf(_x, loc=_mean, scale=_std), color=fig_dict[_set]['color'])\n",
    "                print('Return period (gaussian-fit):', _ds[_obs].sel(return_period='normal', dataset=_set).values)\n",
    "\n",
    "            for iObs, _obs in enumerate(observations):\n",
    "                if _tempo in observations[_obs].keys():\n",
    "                    _text = ('{0:.0f} years'.format(_ds[_obs].sel(return_period='computed', dataset=_set)))\n",
    "                    if bootstrap:\n",
    "                        try:\n",
    "                            _text += (\n",
    "                                ' [{0:.0f} - '.format(min(_ds[_obs].sel(return_period='computed', dataset=_set)-1, _ds[_obs].sel(return_period='bootstrap-low', dataset=_set)))\n",
    "                                +'{0:.0f}]'.format(max(_ds[_obs].sel(return_period='computed', dataset=_set)+1, _ds[_obs].sel(return_period='bootstrap-high', dataset=_set)))\n",
    "                                )\n",
    "                        except OverflowError:\n",
    "                            pass\n",
    "                    rtn_box.append(Line2D([0], [0], label=_text, ls=linestyles[iObs], color=fig_dict[_set]['color'], lw=lw))\n",
    "\n",
    "        for iObs, _obs in enumerate(observations):\n",
    "            if _tempo in observations[_obs].keys():\n",
    "                ax[i].axvline(x=_wmo, ls=linestyles[iObs], c='red', lw=lw,\n",
    "                )\n",
    "                ax[i].text(_wmo, 0, ' '+_year+' '+variable_dict[variable]+' Obs. = {:.2f}°C'.format(_wmo), fontsize=12, c='red', va='bottom', ha='right', rotation=90)\n",
    "                ax[i].add_artist(ax[i].legend(handles=rtn_box, loc='upper right', fontsize=12))\n",
    "\n",
    "        ax[i].set_title(_p+'Year '+_year+' at ANT_GWL = {:.2f}°C'.format(_gwl), fontsize=15)\n",
    "        ax[i].set_xlabel(temporality_dict[_tempo]+' '+variable_dict[variable]+' anomaly (°C), reference period: '+piStr+'-'+piEnd, fontsize=15)\n",
    "        ax[i].set_ylabel('Occurrence (%)', fontsize=15)\n",
    "        ax[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "        ax[i].axvline(x=0, ls='-', c='k', lw=.5)\n",
    "\n",
    "    _h, _l = ax[i].get_legend_handles_labels()\n",
    "    for j, _l1 in enumerate(_l):\n",
    "        if _l1 not in l:\n",
    "            h += [_h[j]] ; l += [_l1]\n",
    "    i += 1\n",
    "\n",
    "\n",
    "_lgd = ax[0].legend(h, l, fontsize=12, loc='upper left') # 15\n",
    "\n",
    "_saveFig = outDir+'yr'.join(saveName.split('tempo'))+'/'+'forster2025_fig8_cassou_line'+extension\n",
    "plt.savefig(_saveFig, transparent=True)\n",
    "print('Saved:', _saveFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modes of internal variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsat_wmo = {'2022': 1.15, '2023': 1.44, '2024': 1.52}\n",
    "show_quantiles = False ; show_obs = True\n",
    "vmin = 0.9 ; vmax = 2.0 ; nbins = (vmax - vmin) / 0.1\n",
    "\n",
    "_driver_members = ds_driver.member.values\n",
    "for _driver in ds_driver:\n",
    "    _driver_members = list(set(_driver_members).intersection(set(ds_driver.get(_driver).dropna(dim='member', how='all').member.values)))\n",
    "_kept_driver_members = list(set(_driver_members).intersection(set(xr.where(ds_mask['cnstMod'], 1, np.nan).dropna(dim='member').member.values)))\n",
    "print(len(_kept_driver_members), 'kept members have all drivers in common.')\n",
    "_LOCdata = ds_GWL.get(variable+'_kMod').isel(temporality = 0, warming_level = 0).sel(member = _kept_driver_members)\n",
    "_LOCweights = local_weights(_LOCdata, method='occurrence-per-model')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7.5))\n",
    "ax.set_title(variable_dict[variable]+' interannual anomalies from CMIP6 (15 models) at ANT_GWL = {:.2f}°C\\nconditional to phases of OND [yr-1] ENSO and AMV modes of variability'.format(_LOCdata.warming_level), fontsize=15)\n",
    "ax.set_xlabel('Annual yr[0] demeaned AMV (°C)', fontsize=15) ; ax.set_ylabel('OND yr[-1] demeaned Niño 3.4 (°C)', fontsize=15)\n",
    "ax.axvline(x=0,lw=.75,c='k') ; ax.axhline(y=0,lw=.75,c='k')\n",
    "ax.set_xlim(-0.6, 0.6)\n",
    "ax.set_ylim(-3.75, 3.75)\n",
    "_cmap = get_diverging_cmap()\n",
    "_norm = mclrs.BoundaryNorm(mtick.MaxNLocator(nbins=nbins).tick_values(vmin, vmax), ncolors=_cmap.N, clip=True)\n",
    "\n",
    "sc = plt.scatter(\n",
    "    ds_driver.get('amv').sel(member = _kept_driver_members), ds_driver.get('nino34').sel(member = _kept_driver_members),\n",
    "    c = _LOCdata,\n",
    "    s = 15, cmap = _cmap, alpha = 0.75, norm=_norm,\n",
    ")\n",
    "cb = fig.colorbar(sc, label='°C')\n",
    "cb.set_label(label=temporality_dict[_tempo]+' yr[0] '+variable_dict[variable]+' anomaly (°C),\\nreference period: '+piStr+'-'+piEnd, size=15)\n",
    "\n",
    "_handles = list()\n",
    "\n",
    "if show_obs:\n",
    "    for _year in gsat_wmo:\n",
    "        x = ds_driver_obs.amvGlob.sel(time=_year)\n",
    "        y = ds_driver_obs.nino34.sel(time=_year)\n",
    "        plt.plot(x, y, c=_cmap(_norm(gsat_wmo[_year])), marker='o', mec='k', ms=15)\n",
    "        _va = 'top' if _year == '2022' else 'bottom'\n",
    "        plt.text(x, y, '   '+_year, va = _va, fontsize=12)\n",
    "    _handles.append(Line2D([0], [0], color='w', linestyle='', marker='o', mec='k', ms=15, label='Obs: ERSSTv5/WMO'))\n",
    "\n",
    "if show_quantiles:\n",
    "    nquantiles = 10\n",
    "    for i in range(nquantiles):\n",
    "        zmin = _LOCdata.weighted(_LOCweights).quantile(i/nquantiles)\n",
    "        zmax = _LOCdata.weighted(_LOCweights).quantile((i+1)/nquantiles)\n",
    "\n",
    "        x, y, z = get_interval_averages(\n",
    "            x = ds_driver.get('amv').sel(member = _kept_driver_members),\n",
    "            y = ds_driver.get('nino34').sel(member = _kept_driver_members),\n",
    "            z = ds_GWL.get(variable).isel(temporality = 0, warming_level = 0).sel(member = _kept_driver_members),\n",
    "            zmin = zmin, zmax = zmax)\n",
    "        plt.plot(x, y, c=_cmap(_norm(z)), marker='*', mec='k', ms=15, alpha = 0.75)\n",
    "    _handles.append(Line2D([0], [0], color='w', linestyle='', marker='*', mec='k', ms=15, label='GSAT deciles'))\n",
    "\n",
    "_handles.append(Line2D([0], [0], color='grey', linestyle='', marker='o', ms=5, label='CMIP6 occurrence'))\n",
    "if len(_handles) != 0:\n",
    "    ax.legend(handles=_handles, loc='lower right', fontsize=12)\n",
    "\n",
    "_saveFig = outDir+'yr'.join(saveName.split('tempo'))+'/'+'forster2025_SI_cassou_line'+extension\n",
    "#plt.savefig(_saveFig, transparent=True)\n",
    "#print('Saved:', _saveFig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('My work is done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
